{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "class A2C():\n",
    "    def __init__(self,\n",
    "                 actor_critic,\n",
    "                 value_loss_coef,\n",
    "                 entropy_coef,\n",
    "                 lr=None,\n",
    "                 eps=None,\n",
    "                 alpha=None,\n",
    "                 max_grad_norm=None):\n",
    "\n",
    "        self.summary_writer = SummaryWriter()\n",
    "        self.actor_critic = actor_critic\n",
    "        \n",
    "\n",
    "        self.value_loss_coef = value_loss_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "\n",
    "        \n",
    "        self.optimizer = optim.RMSprop(\n",
    "            actor_critic.parameters(), lr, eps=eps, alpha=alpha)\n",
    "\n",
    "    def update(self, rollouts, episode_num):\n",
    "        obs_shape = rollouts.obs.size()[2:]\n",
    "        action_shape = rollouts.actions.size()[-1]\n",
    "        num_steps, num_processes, _ = rollouts.rewards.size()\n",
    "\n",
    "        values, action_log_probs, dist_entropy, _ = self.actor_critic.evaluate_actions(\n",
    "            rollouts.obs[:-1].view(-1, *obs_shape),\n",
    "            rollouts.recurrent_hidden_states[0].view(\n",
    "                -1, self.actor_critic.recurrent_hidden_state_size),\n",
    "            rollouts.masks[:-1].view(-1, 1),\n",
    "            rollouts.actions.view(-1, action_shape))\n",
    "\n",
    "        values = values.view(num_steps, num_processes, 1)\n",
    "        action_log_probs = action_log_probs.view(num_steps, num_processes, 1)\n",
    "\n",
    "        advantages = rollouts.returns[:-1] - values\n",
    "        value_loss = advantages.pow(2).mean()\n",
    "\n",
    "        action_loss = -(advantages.detach() * action_log_probs).mean()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        (value_loss * self.value_loss_coef + action_loss -\n",
    "         dist_entropy * self.entropy_coef).backward()\n",
    "\n",
    "        self.summary_writer.add_scalar('value_loss', value_loss.item(), episode_num)\n",
    "        self.summary_writer.add_scalar('action_loss', action_loss.item(), episode_num)\n",
    "        self.summary_writer.add_scalar('dist_entropy', dist_entropy.item(), episode_num)\n",
    "        nn.utils.clip_grad_norm_(self.actor_critic.parameters(),\n",
    "                                     self.max_grad_norm)\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return value_loss.item(), action_loss.item(), dist_entropy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
    "\n",
    "\n",
    "def _flatten_helper(T, N, _tensor):\n",
    "    return _tensor.view(T * N, *_tensor.size()[2:])\n",
    "\n",
    "\n",
    "class RolloutStorage(object):\n",
    "    def __init__(self, num_steps, num_processes, obs_shape, action_space,\n",
    "                 recurrent_hidden_state_size):\n",
    "        self.obs = torch.zeros(num_steps + 1, num_processes, *obs_shape)\n",
    "        self.recurrent_hidden_states = torch.zeros(\n",
    "            num_steps + 1, num_processes, recurrent_hidden_state_size)\n",
    "        self.rewards = torch.zeros(num_steps, num_processes, 1)\n",
    "        self.value_preds = torch.zeros(num_steps + 1, num_processes, 1)\n",
    "        self.returns = torch.zeros(num_steps + 1, num_processes, 1)\n",
    "        self.action_log_probs = torch.zeros(num_steps, num_processes, 1)\n",
    "        #if action_space.__class__.__name__ == 'Discrete':\n",
    "        action_shape = 1\n",
    "        #else:\n",
    "        #    action_shape = action_space.shape[0]\n",
    "        self.actions = torch.zeros(num_steps, num_processes, action_shape)\n",
    "        if action_space.__class__.__name__ == 'Discrete':\n",
    "            self.actions = self.actions.long()\n",
    "        self.masks = torch.ones(num_steps + 1, num_processes, 1)\n",
    "\n",
    "        # Masks that indicate whether it's a true terminal state\n",
    "        # or time limit end state\n",
    "        self.bad_masks = torch.ones(num_steps + 1, num_processes, 1)\n",
    "\n",
    "        self.num_steps = num_steps\n",
    "        self.step = 0\n",
    "\n",
    "    def to(self, device):\n",
    "        self.obs = self.obs.to(device)\n",
    "        self.recurrent_hidden_states = self.recurrent_hidden_states.to(device)\n",
    "        self.rewards = self.rewards.to(device)\n",
    "        self.value_preds = self.value_preds.to(device)\n",
    "        self.returns = self.returns.to(device)\n",
    "        self.action_log_probs = self.action_log_probs.to(device)\n",
    "        self.actions = self.actions.to(device)\n",
    "        self.masks = self.masks.to(device)\n",
    "        self.bad_masks = self.bad_masks.to(device)\n",
    "\n",
    "    def insert(self, obs, recurrent_hidden_states, actions, action_log_probs,\n",
    "               value_preds, rewards, masks, bad_masks):\n",
    "        self.obs[self.step + 1].copy_(obs)\n",
    "        self.recurrent_hidden_states[self.step +\n",
    "                                     1].copy_(recurrent_hidden_states)\n",
    "        self.actions[self.step].copy_(actions)\n",
    "        self.action_log_probs[self.step].copy_(action_log_probs)\n",
    "        self.value_preds[self.step].copy_(value_preds)\n",
    "        self.rewards[self.step].copy_(rewards)\n",
    "        self.masks[self.step + 1].copy_(masks)\n",
    "        self.bad_masks[self.step + 1].copy_(bad_masks)\n",
    "\n",
    "        self.step = (self.step + 1) % self.num_steps\n",
    "\n",
    "    def after_update(self):\n",
    "        self.obs[0].copy_(self.obs[-1])\n",
    "        self.recurrent_hidden_states[0].copy_(self.recurrent_hidden_states[-1])\n",
    "        self.masks[0].copy_(self.masks[-1])\n",
    "        self.bad_masks[0].copy_(self.bad_masks[-1])\n",
    "\n",
    "    def compute_returns(self,\n",
    "                        next_value,\n",
    "                        use_gae,\n",
    "                        gamma,\n",
    "                        gae_lambda,\n",
    "                        use_proper_time_limits=True):\n",
    "        if use_proper_time_limits:\n",
    "            if use_gae:\n",
    "                self.value_preds[-1] = next_value\n",
    "                gae = 0\n",
    "                for step in reversed(range(self.rewards.size(0))):\n",
    "                    delta = self.rewards[step] + gamma * self.value_preds[\n",
    "                        step + 1] * self.masks[step +\n",
    "                                               1] - self.value_preds[step]\n",
    "                    gae = delta + gamma * gae_lambda * self.masks[step +\n",
    "                                                                  1] * gae\n",
    "                    gae = gae * self.bad_masks[step + 1]\n",
    "                    self.returns[step] = gae + self.value_preds[step]\n",
    "            else:\n",
    "                self.returns[-1] = next_value\n",
    "                for step in reversed(range(self.rewards.size(0))):\n",
    "                    self.returns[step] = (self.returns[step + 1] * \\\n",
    "                        gamma * self.masks[step + 1] + self.rewards[step]) * self.bad_masks[step + 1] \\\n",
    "                        + (1 - self.bad_masks[step + 1]) * self.value_preds[step]\n",
    "        else:\n",
    "            if use_gae:\n",
    "                self.value_preds[-1] = next_value\n",
    "                gae = 0\n",
    "                for step in reversed(range(self.rewards.size(0))):\n",
    "                    delta = self.rewards[step] + gamma * self.value_preds[\n",
    "                        step + 1] * self.masks[step +\n",
    "                                               1] - self.value_preds[step]\n",
    "                    gae = delta + gamma * gae_lambda * self.masks[step +\n",
    "                                                                  1] * gae\n",
    "                    self.returns[step] = gae + self.value_preds[step]\n",
    "            else:\n",
    "                self.returns[-1] = next_value\n",
    "                for step in reversed(range(self.rewards.size(0))):\n",
    "                    self.returns[step] = self.returns[step + 1] * \\\n",
    "                        gamma * self.masks[step + 1] + self.rewards[step]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_STEP = 850  # Cap the number of steps to run for each env\n",
    "NUM_PROCESSES = 1  # Number of processes to use\n",
    "ENV_DIM = 4  # Dimension of one size of the building zone in the env\n",
    "NUM_ACTION = 8  # Number of actions in the action space\n",
    "NUM_EPISODE = 1000\n",
    "NUM_STEP_TOTAL = NUM_STEP * NUM_EPISODE \n",
    "GAMMA = 0.8  # Discount factor\n",
    "VALUE_COEFF = 0.15  # Value loss coefficient\n",
    "ENTROPY_COEFF = 0.05  # Entropy regularisation coefficient\n",
    "LR = 0.0000625\n",
    "RMS_EPSILON = 0.1\n",
    "RMS_ALPHA = 0.99\n",
    "\n",
    "RECURRENT_HIDDEN_SIZE = 256  # Size of the hidden state in the actor-critic model\n",
    "USE_LR_DECAY = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init(module, weight_init, bias_init, gain=1):\n",
    "    weight_init(module.weight.data, gain=gain)\n",
    "    bias_init(module.bias.data)\n",
    "    return module\n",
    "\n",
    "# Categorical\n",
    "class FixedCategorical(torch.distributions.Categorical):\n",
    "    def sample(self):\n",
    "        return super().sample().unsqueeze(-1)\n",
    "\n",
    "    def log_probs(self, actions):\n",
    "        return (\n",
    "            super()\n",
    "            .log_prob(actions.squeeze(-1))\n",
    "            .view(actions.size(0), -1)\n",
    "            .sum(-1)\n",
    "            .unsqueeze(-1)\n",
    "        )\n",
    "\n",
    "    def mode(self):\n",
    "        return self.probs.argmax(dim=-1, keepdim=True)\n",
    "\n",
    "class Categorical(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super(Categorical, self).__init__()\n",
    "\n",
    "        init_ = lambda m: init(\n",
    "            m,\n",
    "            nn.init.orthogonal_,\n",
    "            lambda x: nn.init.constant_(x, 0),\n",
    "            gain=0.01)\n",
    "\n",
    "        self.linear = init_(nn.Linear(num_inputs, num_outputs))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return FixedCategorical(logits=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, obs_shape, action_space, base, base_kwargs=None):\n",
    "        super(Policy, self).__init__()\n",
    "        if base_kwargs is None:\n",
    "            base_kwargs = {}\n",
    "\n",
    "        self.base = base(obs_shape[-1], **base_kwargs)\n",
    "        #print(\"action_space\", action_space.__class__.__name__)\n",
    "        #if action_space.__class__.__name__ == \"Discrete\":\n",
    "        num_outputs = NUM_ACTION\n",
    "        self.dist = Categorical(self.base.output_size, num_outputs)\n",
    "        #else:\n",
    "           # raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def is_recurrent(self):\n",
    "        return self.base.is_recurrent\n",
    "\n",
    "    @property\n",
    "    def recurrent_hidden_state_size(self):\n",
    "        \"\"\"Size of rnn_hx.\"\"\"\n",
    "        return self.base.recurrent_hidden_state_size\n",
    "\n",
    "    def forward(self, inputs, rnn_hxs, masks):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def act(self, inputs, rnn_hxs, masks, deterministic=False):\n",
    "        value, actor_features, rnn_hxs = self.base(inputs, rnn_hxs, masks)\n",
    "        dist = self.dist(actor_features)\n",
    "\n",
    "        if deterministic:\n",
    "            action = dist.mode()\n",
    "        else:\n",
    "            action = dist.sample()\n",
    "\n",
    "        action_log_probs = dist.log_probs(action)\n",
    "        #dist_entropy = dist.entropy().mean()\n",
    "\n",
    "        return value, action, action_log_probs, rnn_hxs\n",
    "\n",
    "    def get_value(self, inputs, rnn_hxs, masks):\n",
    "        value, _, _ = self.base(inputs, rnn_hxs, masks)\n",
    "        return value\n",
    "\n",
    "    def evaluate_actions(self, inputs, rnn_hxs, masks, action):\n",
    "        value, actor_features, rnn_hxs = self.base(inputs, rnn_hxs, masks)\n",
    "        dist = self.dist(actor_features)\n",
    "\n",
    "        action_log_probs = dist.log_probs(action)\n",
    "        dist_entropy = dist.entropy().mean()\n",
    "\n",
    "        return value, action_log_probs, dist_entropy, rnn_hxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNBase(nn.Module):\n",
    "    def __init__(self, recurrent, recurrent_input_size, hidden_size):\n",
    "        super(NNBase, self).__init__()\n",
    "\n",
    "        self._hidden_size = hidden_size\n",
    "        self._recurrent = recurrent\n",
    "\n",
    "        if recurrent:\n",
    "            self.gru = nn.GRU(recurrent_input_size, hidden_size)\n",
    "            for name, param in self.gru.named_parameters():\n",
    "                if 'bias' in name:\n",
    "                    nn.init.constant_(param, 0)\n",
    "                elif 'weight' in name:\n",
    "                    nn.init.orthogonal_(param)\n",
    "\n",
    "    @property\n",
    "    def is_recurrent(self):\n",
    "        return self._recurrent\n",
    "\n",
    "    @property\n",
    "    def recurrent_hidden_state_size(self):\n",
    "        if self._recurrent:\n",
    "            return self._hidden_size\n",
    "        return 1\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._hidden_size\n",
    "\n",
    "    def _forward_gru(self, x, hxs, masks):\n",
    "        if x.size(0) == hxs.size(0):\n",
    "            x, hxs = self.gru(x.unsqueeze(0), (hxs * masks).unsqueeze(0))\n",
    "            x = x.squeeze(0)\n",
    "            hxs = hxs.squeeze(0)\n",
    "        else:\n",
    "            # x is a (T, N, -1) tensor that has been flatten to (T * N, -1)\n",
    "            N = hxs.size(0)\n",
    "            T = int(x.size(0) / N)\n",
    "\n",
    "            # unflatten\n",
    "            x = x.view(T, N, x.size(1))\n",
    "\n",
    "            # Same deal with masks\n",
    "            masks = masks.view(T, N)\n",
    "\n",
    "            # Let's figure out which steps in the sequence have a zero for any agent\n",
    "            # We will always assume t=0 has a zero in it as that makes the logic cleaner\n",
    "            has_zeros = ((masks[1:] == 0.0) \\\n",
    "                            .any(dim=-1)\n",
    "                            .nonzero()\n",
    "                            .squeeze()\n",
    "                            .cpu())\n",
    "\n",
    "            # +1 to correct the masks[1:]\n",
    "            if has_zeros.dim() == 0:\n",
    "                # Deal with scalar\n",
    "                has_zeros = [has_zeros.item() + 1]\n",
    "            else:\n",
    "                has_zeros = (has_zeros + 1).numpy().tolist()\n",
    "\n",
    "            # add t=0 and t=T to the list\n",
    "            has_zeros = [0] + has_zeros + [T]\n",
    "\n",
    "            hxs = hxs.unsqueeze(0)\n",
    "            outputs = []\n",
    "            for i in range(len(has_zeros) - 1):\n",
    "                # We can now process steps that don't have any zeros in masks together!\n",
    "                # This is much faster\n",
    "                start_idx = has_zeros[i]\n",
    "                end_idx = has_zeros[i + 1]\n",
    "\n",
    "                rnn_scores, hxs = self.gru(\n",
    "                    x[start_idx:end_idx],\n",
    "                    hxs * masks[start_idx].view(1, -1, 1))\n",
    "\n",
    "                outputs.append(rnn_scores)\n",
    "\n",
    "            # assert len(outputs) == T\n",
    "            # x is a (T, N, -1) tensor\n",
    "            x = torch.cat(outputs, dim=0)\n",
    "            # flatten\n",
    "            x = x.view(T * N, -1)\n",
    "            hxs = hxs.squeeze(0)\n",
    "\n",
    "        return x, hxs\n",
    "\n",
    "class CNNBase(NNBase):\n",
    "    def __init__(self, num_inputs, recurrent=False, hidden_size=1024):\n",
    "        super(CNNBase, self).__init__(recurrent, hidden_size, hidden_size)\n",
    "\n",
    "        init_ = lambda m: init(m, nn.init.orthogonal_, lambda x: nn.init.\n",
    "                               constant_(x, 0), nn.init.calculate_gain('relu'))\n",
    "\n",
    "        \n",
    "        self.conv1 = init_(nn.Conv3d(3, 29, 3, 1, 1))\n",
    "        self.conv2 = init_(nn.Conv3d(32, 67, 3, 1, 1))\n",
    "        self.fc1 = init_(nn.Linear((67 + 3) * ENV_DIM * ENV_DIM * ENV_DIM, 1024))\n",
    "        \n",
    "        init_ = lambda m: init(m, nn.init.orthogonal_, lambda x: nn.init.\n",
    "                               constant_(x, 0))\n",
    "        self.critic_linear = init_(nn.Linear(hidden_size, 1))\n",
    "\n",
    "        self.train()\n",
    "\n",
    "    def forward(self, inputs, rnn_hxs, masks):\n",
    "        original_input = inputs\n",
    "        \n",
    "        x = self.conv1(original_input)\n",
    "        x = torch.cat((x, original_input), dim=1)\n",
    "        x = nn.ReLU()(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = torch.cat((x, original_input), dim=1)\n",
    "        x = nn.ReLU()(x)\n",
    "        #print(x.shape)\n",
    "        x = nn.Flatten()(x)\n",
    "        x = self.fc1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        \n",
    "        \n",
    "        if self.is_recurrent:\n",
    "            x, rnn_hxs = self._forward_gru(x, rnn_hxs, masks)\n",
    "\n",
    "        return self.critic_linear(x), x, rnn_hxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 0, num timesteps 850, FPS 188 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 7.803910732269287 action_loss -5.696107864379883 dist_entropy 2.079435110092163\n",
      "Updates 5, num timesteps 5100, FPS 213 \n",
      " Last 10 training episodes: mean/median reward -0.6/-0.6, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 8.164088249206543 action_loss -5.85037088394165 dist_entropy 2.079429864883423\n",
      "Updates 10, num timesteps 9350, FPS 218 \n",
      " Last 10 training episodes: mean/median reward -0.4/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 6.6808085441589355 action_loss -5.2326436042785645 dist_entropy 2.0794241428375244\n",
      "Updates 15, num timesteps 13600, FPS 221 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 6.401698589324951 action_loss -5.144961833953857 dist_entropy 2.0793774127960205\n",
      "Updates 20, num timesteps 17850, FPS 216 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 5.7413177490234375 action_loss -4.861557483673096 dist_entropy 2.0793344974517822\n",
      "Updates 25, num timesteps 22100, FPS 221 \n",
      " Last 10 training episodes: mean/median reward -0.5/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 4.824771404266357 action_loss -4.446001052856445 dist_entropy 2.079298496246338\n",
      "Updates 30, num timesteps 26350, FPS 222 \n",
      " Last 10 training episodes: mean/median reward -0.4/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 3.3864152431488037 action_loss -3.6925535202026367 dist_entropy 2.0791473388671875\n",
      "Updates 35, num timesteps 30600, FPS 222 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 2.813084125518799 action_loss -3.3367514610290527 dist_entropy 2.0790188312530518\n",
      "Updates 40, num timesteps 34850, FPS 220 \n",
      " Last 10 training episodes: mean/median reward -0.4/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 2.740095615386963 action_loss -3.2725906372070312 dist_entropy 2.079042434692383\n",
      "Updates 45, num timesteps 39100, FPS 220 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 2.7303953170776367 action_loss -3.260082244873047 dist_entropy 2.079007625579834\n",
      "Updates 50, num timesteps 43350, FPS 222 \n",
      " Last 10 training episodes: mean/median reward -0.4/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 1.5179508924484253 action_loss -2.322784185409546 dist_entropy 2.078435182571411\n",
      "Updates 55, num timesteps 47600, FPS 224 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -0.5/-0.2\n",
      "\n",
      "value_loss 1.5476363897323608 action_loss -2.357391595840454 dist_entropy 2.078644275665283\n",
      "Updates 60, num timesteps 51850, FPS 226 \n",
      " Last 10 training episodes: mean/median reward -0.5/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 1.0246492624282837 action_loss -1.8641096353530884 dist_entropy 2.078075408935547\n",
      "Updates 65, num timesteps 56100, FPS 224 \n",
      " Last 10 training episodes: mean/median reward -0.4/-0.2, min/max reward -1.0/0.0\n",
      "\n",
      "value_loss 0.7935665845870972 action_loss -1.491396188735962 dist_entropy 2.0776050090789795\n",
      "Updates 70, num timesteps 60350, FPS 226 \n",
      " Last 10 training episodes: mean/median reward -0.6/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.7808781862258911 action_loss -1.543123483657837 dist_entropy 2.0775089263916016\n",
      "Updates 75, num timesteps 64600, FPS 228 \n",
      " Last 10 training episodes: mean/median reward -0.4/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.9150909185409546 action_loss -1.6447018384933472 dist_entropy 2.0775105953216553\n",
      "Updates 80, num timesteps 68850, FPS 227 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.781613290309906 action_loss -1.5074769258499146 dist_entropy 2.076876640319824\n",
      "Updates 85, num timesteps 73100, FPS 226 \n",
      " Last 10 training episodes: mean/median reward -0.4/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.30417609214782715 action_loss -0.43558627367019653 dist_entropy 2.075343132019043\n",
      "Updates 90, num timesteps 77350, FPS 226 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/0.0\n",
      "\n",
      "value_loss 0.25584232807159424 action_loss -0.25741279125213623 dist_entropy 2.075012445449829\n",
      "Updates 95, num timesteps 81600, FPS 227 \n",
      " Last 10 training episodes: mean/median reward -0.5/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.2027830332517624 action_loss -0.1801721304655075 dist_entropy 2.074160099029541\n",
      "Updates 100, num timesteps 85850, FPS 227 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.47265830636024475 action_loss -0.9733548760414124 dist_entropy 2.074753999710083\n",
      "Updates 105, num timesteps 90100, FPS 228 \n",
      " Last 10 training episodes: mean/median reward -0.5/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.5335946083068848 action_loss -1.1667553186416626 dist_entropy 2.075674057006836\n",
      "Updates 110, num timesteps 94350, FPS 227 \n",
      " Last 10 training episodes: mean/median reward -0.5/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.31059983372688293 action_loss -0.34093010425567627 dist_entropy 2.0722928047180176\n",
      "Updates 115, num timesteps 98600, FPS 225 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.4574819505214691 action_loss -0.9782944321632385 dist_entropy 2.074221611022949\n",
      "Updates 120, num timesteps 102850, FPS 225 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.3705177903175354 action_loss -0.7560986280441284 dist_entropy 2.071836471557617\n",
      "Updates 125, num timesteps 107100, FPS 226 \n",
      " Last 10 training episodes: mean/median reward -0.6/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.2915661931037903 action_loss -0.5848141312599182 dist_entropy 2.071502447128296\n",
      "Updates 130, num timesteps 111350, FPS 227 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.33083048462867737 action_loss 0.49457862973213196 dist_entropy 2.0686402320861816\n",
      "Updates 135, num timesteps 115600, FPS 229 \n",
      " Last 10 training episodes: mean/median reward -0.4/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.3039850890636444 action_loss -0.4657074809074402 dist_entropy 2.0685534477233887\n",
      "Updates 140, num timesteps 119850, FPS 230 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.22750340402126312 action_loss -0.26273250579833984 dist_entropy 2.067700147628784\n",
      "Updates 145, num timesteps 124100, FPS 231 \n",
      " Last 10 training episodes: mean/median reward -0.4/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.22827118635177612 action_loss -0.08885356783866882 dist_entropy 2.066514492034912\n",
      "Updates 150, num timesteps 128350, FPS 232 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.23964154720306396 action_loss -0.04745898395776749 dist_entropy 2.064948320388794\n",
      "Updates 155, num timesteps 132600, FPS 233 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.3818240165710449 action_loss -0.726905107498169 dist_entropy 2.0667271614074707\n",
      "Updates 160, num timesteps 136850, FPS 234 \n",
      " Last 10 training episodes: mean/median reward -0.4/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.26643022894859314 action_loss 0.3554328978061676 dist_entropy 2.0614476203918457\n",
      "Updates 165, num timesteps 141100, FPS 235 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.26922473311424255 action_loss -0.4831726551055908 dist_entropy 2.069088935852051\n",
      "Updates 170, num timesteps 145350, FPS 236 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.22251880168914795 action_loss 0.09218717366456985 dist_entropy 2.0635476112365723\n",
      "Updates 175, num timesteps 149600, FPS 237 \n",
      " Last 10 training episodes: mean/median reward -0.4/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.3597189486026764 action_loss 0.03560623526573181 dist_entropy 2.065791130065918\n",
      "Updates 180, num timesteps 153850, FPS 238 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.28617072105407715 action_loss -0.054629143327474594 dist_entropy 2.059872627258301\n",
      "Updates 185, num timesteps 158100, FPS 238 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/0.0\n",
      "\n",
      "value_loss 0.2972487807273865 action_loss -0.5666426420211792 dist_entropy 2.066816568374634\n",
      "Updates 190, num timesteps 162350, FPS 239 \n",
      " Last 10 training episodes: mean/median reward -0.4/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.2442924529314041 action_loss 0.43396633863449097 dist_entropy 2.0554211139678955\n",
      "Updates 195, num timesteps 166600, FPS 240 \n",
      " Last 10 training episodes: mean/median reward -0.4/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.2477860003709793 action_loss 0.3597833514213562 dist_entropy 2.0545260906219482\n",
      "Updates 200, num timesteps 170850, FPS 241 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.289646178483963 action_loss -0.2706499695777893 dist_entropy 2.0590646266937256\n",
      "Updates 205, num timesteps 175100, FPS 241 \n",
      " Last 10 training episodes: mean/median reward -0.4/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.24914509057998657 action_loss -0.5342503786087036 dist_entropy 2.06343674659729\n",
      "Updates 210, num timesteps 179350, FPS 240 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.24577409029006958 action_loss -0.08723009377717972 dist_entropy 2.061584711074829\n",
      "Updates 215, num timesteps 183600, FPS 239 \n",
      " Last 10 training episodes: mean/median reward -0.4/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.22218108177185059 action_loss 0.43513554334640503 dist_entropy 2.0508460998535156\n",
      "Updates 220, num timesteps 187850, FPS 238 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.22541025280952454 action_loss -0.34324130415916443 dist_entropy 2.0554871559143066\n",
      "Updates 225, num timesteps 192100, FPS 237 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.21841824054718018 action_loss -0.3831526041030884 dist_entropy 2.0559945106506348\n",
      "Updates 230, num timesteps 196350, FPS 237 \n",
      " Last 10 training episodes: mean/median reward -0.5/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.23320667445659637 action_loss -0.23691235482692719 dist_entropy 2.0521841049194336\n",
      "Updates 235, num timesteps 200600, FPS 237 \n",
      " Last 10 training episodes: mean/median reward -0.4/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.29267367720603943 action_loss 0.6234400868415833 dist_entropy 2.0443129539489746\n",
      "Updates 240, num timesteps 204850, FPS 238 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.20327094197273254 action_loss -0.10421133786439896 dist_entropy 2.050548553466797\n",
      "Updates 245, num timesteps 209100, FPS 239 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.1968412846326828 action_loss 0.126359224319458 dist_entropy 2.0477194786071777\n",
      "Updates 250, num timesteps 213350, FPS 239 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.23861990869045258 action_loss 0.299765408039093 dist_entropy 2.043182134628296\n",
      "Updates 255, num timesteps 217600, FPS 240 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.21877576410770416 action_loss -0.1088583692908287 dist_entropy 2.048393726348877\n",
      "Updates 260, num timesteps 221850, FPS 240 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.18645770847797394 action_loss -0.15906837582588196 dist_entropy 2.0525472164154053\n",
      "Updates 265, num timesteps 226100, FPS 241 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.18296493589878082 action_loss -0.2420760989189148 dist_entropy 2.048351764678955\n",
      "Updates 270, num timesteps 230350, FPS 242 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.17884114384651184 action_loss -0.07013121992349625 dist_entropy 2.045161724090576\n",
      "Updates 275, num timesteps 234600, FPS 242 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.16367323696613312 action_loss 0.1684775948524475 dist_entropy 2.0414369106292725\n",
      "Updates 280, num timesteps 238850, FPS 243 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -0.5/-0.2\n",
      "\n",
      "value_loss 0.18483836948871613 action_loss 0.15359774231910706 dist_entropy 2.0492310523986816\n",
      "Updates 285, num timesteps 243100, FPS 244 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.20842938125133514 action_loss -0.15694549679756165 dist_entropy 2.037994623184204\n",
      "Updates 290, num timesteps 247350, FPS 243 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.29170405864715576 action_loss 0.5667123198509216 dist_entropy 2.0324642658233643\n",
      "Updates 295, num timesteps 251600, FPS 244 \n",
      " Last 10 training episodes: mean/median reward -0.5/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.2539408206939697 action_loss -0.5160271525382996 dist_entropy 2.040679693222046\n",
      "Updates 300, num timesteps 255850, FPS 244 \n",
      " Last 10 training episodes: mean/median reward -0.4/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.24197500944137573 action_loss -0.4135052561759949 dist_entropy 2.0527591705322266\n",
      "Updates 305, num timesteps 260100, FPS 245 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.18678586184978485 action_loss 0.06486640125513077 dist_entropy 2.0381624698638916\n",
      "Updates 310, num timesteps 264350, FPS 245 \n",
      " Last 10 training episodes: mean/median reward -0.4/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.18020282685756683 action_loss -0.16940833628177643 dist_entropy 2.0363070964813232\n",
      "Updates 315, num timesteps 268600, FPS 246 \n",
      " Last 10 training episodes: mean/median reward -0.4/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.18822170794010162 action_loss -0.14005793631076813 dist_entropy 2.0479750633239746\n",
      "Updates 320, num timesteps 272850, FPS 246 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -0.5/-0.2\n",
      "\n",
      "value_loss 0.1863071173429489 action_loss 0.06688222289085388 dist_entropy 2.0313916206359863\n",
      "Updates 325, num timesteps 277100, FPS 247 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.2078857272863388 action_loss -0.10380329936742783 dist_entropy 2.028761148452759\n",
      "Updates 330, num timesteps 281350, FPS 247 \n",
      " Last 10 training episodes: mean/median reward -0.4/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.21737879514694214 action_loss 0.03775405138731003 dist_entropy 2.0283589363098145\n",
      "Updates 335, num timesteps 285600, FPS 248 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.20854678750038147 action_loss -0.3781864643096924 dist_entropy 2.0362565517425537\n",
      "Updates 340, num timesteps 289850, FPS 248 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.17868253588676453 action_loss -0.34814655780792236 dist_entropy 2.039095163345337\n",
      "Updates 345, num timesteps 294100, FPS 249 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.1728287637233734 action_loss 0.09874051064252853 dist_entropy 2.0260086059570312\n",
      "Updates 350, num timesteps 298350, FPS 249 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -0.5/-0.2\n",
      "\n",
      "value_loss 0.18664976954460144 action_loss 0.14886978268623352 dist_entropy 2.0385324954986572\n",
      "Updates 355, num timesteps 302600, FPS 250 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.19382290542125702 action_loss -0.4223242700099945 dist_entropy 2.043872594833374\n",
      "Updates 360, num timesteps 306850, FPS 250 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.1545676440000534 action_loss 0.2530319392681122 dist_entropy 2.017529249191284\n",
      "Updates 365, num timesteps 311100, FPS 251 \n",
      " Last 10 training episodes: mean/median reward -0.4/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.1470959484577179 action_loss 0.016399947926402092 dist_entropy 2.0415804386138916\n",
      "Updates 370, num timesteps 315350, FPS 251 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.21563665568828583 action_loss -0.2413657009601593 dist_entropy 2.0404574871063232\n",
      "Updates 375, num timesteps 319600, FPS 251 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.18230924010276794 action_loss -0.19326046109199524 dist_entropy 2.0209498405456543\n",
      "Updates 380, num timesteps 323850, FPS 251 \n",
      " Last 10 training episodes: mean/median reward -0.5/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.1863911747932434 action_loss -0.2674391567707062 dist_entropy 2.0250208377838135\n",
      "Updates 385, num timesteps 328100, FPS 252 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.13992460072040558 action_loss 0.016378110274672508 dist_entropy 2.0197055339813232\n",
      "Updates 390, num timesteps 332350, FPS 252 \n",
      " Last 10 training episodes: mean/median reward -0.4/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.1964130699634552 action_loss -0.5233266353607178 dist_entropy 2.042714834213257\n",
      "Updates 395, num timesteps 336600, FPS 253 \n",
      " Last 10 training episodes: mean/median reward -0.4/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.12435535341501236 action_loss -0.02628077194094658 dist_entropy 2.023183822631836\n",
      "Updates 400, num timesteps 340850, FPS 253 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -0.5/-0.2\n",
      "\n",
      "value_loss 0.16342051327228546 action_loss -0.28553250432014465 dist_entropy 2.0234830379486084\n",
      "Updates 405, num timesteps 345100, FPS 253 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.16363780200481415 action_loss -0.008310949429869652 dist_entropy 2.0162689685821533\n",
      "Updates 410, num timesteps 349350, FPS 254 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.24176742136478424 action_loss 0.5414658784866333 dist_entropy 2.0013976097106934\n",
      "Updates 415, num timesteps 353600, FPS 254 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.18029755353927612 action_loss -0.30160054564476013 dist_entropy 2.037769317626953\n",
      "Updates 420, num timesteps 357850, FPS 254 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.15504835546016693 action_loss 0.09794899821281433 dist_entropy 2.027956008911133\n",
      "Updates 425, num timesteps 362100, FPS 255 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.15143539011478424 action_loss -0.1302625834941864 dist_entropy 2.0150251388549805\n",
      "Updates 430, num timesteps 366350, FPS 255 \n",
      " Last 10 training episodes: mean/median reward -0.4/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.19425328075885773 action_loss -0.3247823417186737 dist_entropy 2.0340445041656494\n",
      "Updates 435, num timesteps 370600, FPS 255 \n",
      " Last 10 training episodes: mean/median reward -0.5/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.22234319150447845 action_loss -0.5194876194000244 dist_entropy 2.0343782901763916\n",
      "Updates 440, num timesteps 374850, FPS 255 \n",
      " Last 10 training episodes: mean/median reward -0.4/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.18517650663852692 action_loss -0.36545926332473755 dist_entropy 2.0338220596313477\n",
      "Updates 445, num timesteps 379100, FPS 256 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.16527779400348663 action_loss 0.00024292102898471057 dist_entropy 2.014392137527466\n",
      "Updates 450, num timesteps 383350, FPS 256 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -0.5/-0.2\n",
      "\n",
      "value_loss 0.1443714201450348 action_loss -0.0704612284898758 dist_entropy 2.0283377170562744\n",
      "Updates 455, num timesteps 387600, FPS 256 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.21016117930412292 action_loss -0.43512555956840515 dist_entropy 2.0262198448181152\n",
      "Updates 460, num timesteps 391850, FPS 256 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.1701524704694748 action_loss 0.24736963212490082 dist_entropy 2.001734972000122\n",
      "Updates 465, num timesteps 396100, FPS 257 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.1313743144273758 action_loss 0.013910086825489998 dist_entropy 2.0038609504699707\n",
      "Updates 470, num timesteps 400350, FPS 257 \n",
      " Last 10 training episodes: mean/median reward -0.5/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.1263413280248642 action_loss -0.08935084939002991 dist_entropy 2.0103516578674316\n",
      "Updates 475, num timesteps 404600, FPS 257 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.18078529834747314 action_loss 0.2511909306049347 dist_entropy 2.0062599182128906\n",
      "Updates 480, num timesteps 408850, FPS 257 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.15108174085617065 action_loss 0.07435614615678787 dist_entropy 2.0056517124176025\n",
      "Updates 485, num timesteps 413100, FPS 258 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.18239915370941162 action_loss 0.19950158894062042 dist_entropy 2.0178065299987793\n",
      "Updates 490, num timesteps 417350, FPS 258 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.2102099061012268 action_loss 0.4801539182662964 dist_entropy 2.017324686050415\n",
      "Updates 495, num timesteps 421600, FPS 258 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.16718222200870514 action_loss -0.19581760466098785 dist_entropy 2.022088050842285\n",
      "Updates 500, num timesteps 425850, FPS 258 \n",
      " Last 10 training episodes: mean/median reward -0.5/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.13602812588214874 action_loss -0.14763444662094116 dist_entropy 2.0190677642822266\n",
      "Updates 505, num timesteps 430100, FPS 258 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.12513910233974457 action_loss -0.18114610016345978 dist_entropy 2.001678705215454\n",
      "Updates 510, num timesteps 434350, FPS 259 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.16033479571342468 action_loss 0.03350920230150223 dist_entropy 2.01654314994812\n",
      "Updates 515, num timesteps 438600, FPS 259 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.19662639498710632 action_loss 0.11047208309173584 dist_entropy 2.017335891723633\n",
      "Updates 520, num timesteps 442850, FPS 259 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.1504073441028595 action_loss -0.28408533334732056 dist_entropy 1.9917916059494019\n",
      "Updates 525, num timesteps 447100, FPS 259 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.1313449740409851 action_loss -0.23594564199447632 dist_entropy 1.9898778200149536\n",
      "Updates 530, num timesteps 451350, FPS 259 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.13911709189414978 action_loss -0.14370650053024292 dist_entropy 2.0206894874572754\n",
      "Updates 535, num timesteps 455600, FPS 260 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.1494615226984024 action_loss 0.07910359650850296 dist_entropy 1.9977449178695679\n",
      "Updates 540, num timesteps 459850, FPS 260 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.14319361746311188 action_loss -0.2721502482891083 dist_entropy 1.9865351915359497\n",
      "Updates 545, num timesteps 464100, FPS 260 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.1252359002828598 action_loss 0.07183348387479782 dist_entropy 2.012232780456543\n",
      "Updates 550, num timesteps 468350, FPS 260 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.14208310842514038 action_loss -0.1730799376964569 dist_entropy 2.0172436237335205\n",
      "Updates 555, num timesteps 472600, FPS 261 \n",
      " Last 10 training episodes: mean/median reward -0.4/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.09933088719844818 action_loss 0.09361164271831512 dist_entropy 1.9863556623458862\n",
      "Updates 560, num timesteps 476850, FPS 261 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.1423012763261795 action_loss 0.139789879322052 dist_entropy 1.9884933233261108\n",
      "Updates 565, num timesteps 481100, FPS 261 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.16248567402362823 action_loss 0.26634013652801514 dist_entropy 2.009650945663452\n",
      "Updates 570, num timesteps 485350, FPS 261 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.12576618790626526 action_loss 0.08582592755556107 dist_entropy 1.98466956615448\n",
      "Updates 575, num timesteps 489600, FPS 262 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.152230367064476 action_loss -0.22371581196784973 dist_entropy 2.0158655643463135\n",
      "Updates 580, num timesteps 493850, FPS 262 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.14085566997528076 action_loss 0.10993161797523499 dist_entropy 1.9844869375228882\n",
      "Updates 585, num timesteps 498100, FPS 262 \n",
      " Last 10 training episodes: mean/median reward -0.5/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.10248706489801407 action_loss 0.01297059841454029 dist_entropy 2.013676881790161\n",
      "Updates 590, num timesteps 502350, FPS 262 \n",
      " Last 10 training episodes: mean/median reward -0.5/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.09989672899246216 action_loss -0.08326228708028793 dist_entropy 1.9810330867767334\n",
      "Updates 595, num timesteps 506600, FPS 263 \n",
      " Last 10 training episodes: mean/median reward -0.4/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.1658531278371811 action_loss 0.12240564078092575 dist_entropy 2.0057213306427\n",
      "Updates 600, num timesteps 510850, FPS 263 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.10074033588171005 action_loss 0.24149836599826813 dist_entropy 1.9794756174087524\n",
      "Updates 605, num timesteps 515100, FPS 263 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.11321765929460526 action_loss -0.2005244344472885 dist_entropy 1.9764710664749146\n",
      "Updates 610, num timesteps 519350, FPS 263 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.09556108713150024 action_loss -0.0458468459546566 dist_entropy 1.9770102500915527\n",
      "Updates 615, num timesteps 523600, FPS 263 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.11605459451675415 action_loss -0.3023662567138672 dist_entropy 2.0117433071136475\n",
      "Updates 620, num timesteps 527850, FPS 263 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.09021639823913574 action_loss 0.007086473051458597 dist_entropy 1.9747719764709473\n",
      "Updates 625, num timesteps 532100, FPS 264 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.11129380017518997 action_loss -0.18984070420265198 dist_entropy 1.9853638410568237\n",
      "Updates 630, num timesteps 536350, FPS 264 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.11992447078227997 action_loss 0.20081520080566406 dist_entropy 2.002814769744873\n",
      "Updates 635, num timesteps 540600, FPS 264 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.1334027349948883 action_loss -0.0953289344906807 dist_entropy 1.9858498573303223\n",
      "Updates 640, num timesteps 544850, FPS 264 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.08474355936050415 action_loss 0.08255598694086075 dist_entropy 1.9751331806182861\n",
      "Updates 645, num timesteps 549100, FPS 264 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.1350739300251007 action_loss 0.36859577894210815 dist_entropy 1.9629942178726196\n",
      "Updates 650, num timesteps 553350, FPS 265 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.11797934025526047 action_loss -0.15843720734119415 dist_entropy 2.0096635818481445\n",
      "Updates 655, num timesteps 557600, FPS 265 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.15323589742183685 action_loss -0.3084377646446228 dist_entropy 1.9861239194869995\n",
      "Updates 660, num timesteps 561850, FPS 265 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.0898107960820198 action_loss -0.0837709903717041 dist_entropy 1.9807721376419067\n",
      "Updates 665, num timesteps 566100, FPS 265 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.09882799535989761 action_loss -0.1978175938129425 dist_entropy 1.9853349924087524\n",
      "Updates 670, num timesteps 570350, FPS 265 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.07830605655908585 action_loss -0.10798400640487671 dist_entropy 1.9782822132110596\n",
      "Updates 675, num timesteps 574600, FPS 266 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.09478212147951126 action_loss -0.19672396779060364 dist_entropy 1.9712992906570435\n",
      "Updates 680, num timesteps 578850, FPS 266 \n",
      " Last 10 training episodes: mean/median reward -0.4/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.11200166493654251 action_loss 0.09371163696050644 dist_entropy 1.9752869606018066\n",
      "Updates 685, num timesteps 583100, FPS 266 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.15203095972537994 action_loss 0.301838219165802 dist_entropy 1.9581668376922607\n",
      "Updates 690, num timesteps 587350, FPS 266 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.1533183604478836 action_loss 0.13455671072006226 dist_entropy 1.991824746131897\n",
      "Updates 695, num timesteps 591600, FPS 266 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.08952534198760986 action_loss -0.12160813063383102 dist_entropy 1.9651716947555542\n",
      "Updates 700, num timesteps 595850, FPS 266 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.09062673896551132 action_loss 0.1334545910358429 dist_entropy 1.9605252742767334\n",
      "Updates 705, num timesteps 600100, FPS 266 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -0.5/-0.2\n",
      "\n",
      "value_loss 0.07877690345048904 action_loss -0.13550017774105072 dist_entropy 1.9636446237564087\n",
      "Updates 710, num timesteps 604350, FPS 267 \n",
      " Last 10 training episodes: mean/median reward -0.5/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.12015912681818008 action_loss -0.012305863201618195 dist_entropy 1.9937670230865479\n",
      "Updates 715, num timesteps 608600, FPS 267 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.08133073151111603 action_loss 0.02087983675301075 dist_entropy 1.9602409601211548\n",
      "Updates 720, num timesteps 612850, FPS 267 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -0.5/-0.2\n",
      "\n",
      "value_loss 0.12266389280557632 action_loss -0.17427664995193481 dist_entropy 1.9631260633468628\n",
      "Updates 725, num timesteps 617100, FPS 267 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.125868558883667 action_loss -0.03149188309907913 dist_entropy 1.967635154724121\n",
      "Updates 730, num timesteps 621350, FPS 267 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.10169506818056107 action_loss -0.001973350765183568 dist_entropy 1.9697115421295166\n",
      "Updates 735, num timesteps 625600, FPS 267 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.08372718840837479 action_loss 0.17741799354553223 dist_entropy 1.9578410387039185\n",
      "Updates 740, num timesteps 629850, FPS 268 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.1349918246269226 action_loss 0.06388069689273834 dist_entropy 1.9885603189468384\n",
      "Updates 745, num timesteps 634100, FPS 268 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.10143764317035675 action_loss -0.18216216564178467 dist_entropy 1.9632648229599\n",
      "Updates 750, num timesteps 638350, FPS 268 \n",
      " Last 10 training episodes: mean/median reward -0.4/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.10391373187303543 action_loss -0.22201383113861084 dist_entropy 1.9688708782196045\n",
      "Updates 755, num timesteps 642600, FPS 268 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.12160096317529678 action_loss 0.09406859427690506 dist_entropy 1.988417148590088\n",
      "Updates 760, num timesteps 646850, FPS 268 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.0742756649851799 action_loss -0.08761783689260483 dist_entropy 1.9522945880889893\n",
      "Updates 765, num timesteps 651100, FPS 268 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.07294259965419769 action_loss -0.04409181326627731 dist_entropy 1.9651386737823486\n",
      "Updates 770, num timesteps 655350, FPS 268 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.10961794853210449 action_loss 0.037511538714170456 dist_entropy 1.9491719007492065\n",
      "Updates 775, num timesteps 659600, FPS 269 \n",
      " Last 10 training episodes: mean/median reward -0.4/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.07111041992902756 action_loss -0.11462134122848511 dist_entropy 1.9543378353118896\n",
      "Updates 780, num timesteps 663850, FPS 269 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.10633203387260437 action_loss 0.028354015201330185 dist_entropy 1.9491807222366333\n",
      "Updates 785, num timesteps 668100, FPS 269 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.09904836118221283 action_loss -0.05849449336528778 dist_entropy 1.988940954208374\n",
      "Updates 790, num timesteps 672350, FPS 269 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.07309480011463165 action_loss -0.03518235310912132 dist_entropy 1.960017442703247\n",
      "Updates 795, num timesteps 676600, FPS 269 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.09744719415903091 action_loss -0.21246077120304108 dist_entropy 1.9665840864181519\n",
      "Updates 800, num timesteps 680850, FPS 269 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.07215606421232224 action_loss 0.1063465029001236 dist_entropy 1.9472962617874146\n",
      "Updates 805, num timesteps 685100, FPS 269 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.09695858508348465 action_loss -0.08463484793901443 dist_entropy 1.9471185207366943\n",
      "Updates 810, num timesteps 689350, FPS 269 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.09392216801643372 action_loss -0.06994782388210297 dist_entropy 1.9433597326278687\n",
      "Updates 815, num timesteps 693600, FPS 270 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.11777609586715698 action_loss -0.22074559330940247 dist_entropy 1.9824918508529663\n",
      "Updates 820, num timesteps 697850, FPS 270 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.08367157727479935 action_loss -0.10412230342626572 dist_entropy 1.9478110074996948\n",
      "Updates 825, num timesteps 702100, FPS 270 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.08176708966493607 action_loss -0.015292729251086712 dist_entropy 1.941013216972351\n",
      "Updates 830, num timesteps 706350, FPS 270 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.06907370686531067 action_loss 0.15431946516036987 dist_entropy 1.9428277015686035\n",
      "Updates 835, num timesteps 710600, FPS 270 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.07685498893260956 action_loss 0.1443687081336975 dist_entropy 1.9399604797363281\n",
      "Updates 840, num timesteps 714850, FPS 270 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.14042647182941437 action_loss -0.3223998248577118 dist_entropy 1.9798874855041504\n",
      "Updates 845, num timesteps 719100, FPS 270 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.06899294257164001 action_loss -0.09946690499782562 dist_entropy 1.9404288530349731\n",
      "Updates 850, num timesteps 723350, FPS 271 \n",
      " Last 10 training episodes: mean/median reward -0.4/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.06528901308774948 action_loss -0.040849123150110245 dist_entropy 1.9399415254592896\n",
      "Updates 855, num timesteps 727600, FPS 271 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.0885288342833519 action_loss -0.01672162301838398 dist_entropy 1.9583302736282349\n",
      "Updates 860, num timesteps 731850, FPS 271 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.10618807375431061 action_loss 0.19387322664260864 dist_entropy 1.9773993492126465\n",
      "Updates 865, num timesteps 736100, FPS 271 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.08882785588502884 action_loss -0.0923643559217453 dist_entropy 1.9543993473052979\n",
      "Updates 870, num timesteps 740350, FPS 271 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.13141122460365295 action_loss -0.22678889334201813 dist_entropy 1.9768074750900269\n",
      "Updates 875, num timesteps 744600, FPS 271 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.07037653774023056 action_loss -0.06499116867780685 dist_entropy 1.9315834045410156\n",
      "Updates 880, num timesteps 748850, FPS 271 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.08910558372735977 action_loss -0.07159516215324402 dist_entropy 1.9452561140060425\n",
      "Updates 885, num timesteps 753100, FPS 271 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.09519466012716293 action_loss -0.20188848674297333 dist_entropy 1.954626441001892\n",
      "Updates 890, num timesteps 757350, FPS 272 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.07516968250274658 action_loss 0.16378402709960938 dist_entropy 1.9465203285217285\n",
      "Updates 895, num timesteps 761600, FPS 272 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.08305629342794418 action_loss 0.009391168132424355 dist_entropy 1.9442638158798218\n",
      "Updates 900, num timesteps 765850, FPS 272 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.0705692246556282 action_loss -0.05968664586544037 dist_entropy 1.937873125076294\n",
      "Updates 905, num timesteps 770100, FPS 272 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.09952975809574127 action_loss -0.10939253121614456 dist_entropy 1.9733842611312866\n",
      "Updates 910, num timesteps 774350, FPS 272 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.07657395303249359 action_loss -0.19563736021518707 dist_entropy 1.93132483959198\n",
      "Updates 915, num timesteps 778600, FPS 272 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.12547726929187775 action_loss 0.01110946573317051 dist_entropy 1.9700688123703003\n",
      "Updates 920, num timesteps 782850, FPS 271 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.06464002281427383 action_loss -0.12349936366081238 dist_entropy 1.9358989000320435\n",
      "Updates 925, num timesteps 787100, FPS 271 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.08909998089075089 action_loss -0.0599851980805397 dist_entropy 1.9306538105010986\n",
      "Updates 930, num timesteps 791350, FPS 271 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.059646692126989365 action_loss -0.051747407764196396 dist_entropy 1.9308116436004639\n",
      "Updates 935, num timesteps 795600, FPS 271 \n",
      " Last 10 training episodes: mean/median reward -0.4/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.144304558634758 action_loss 0.02554435096681118 dist_entropy 1.9632731676101685\n",
      "Updates 940, num timesteps 799850, FPS 271 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.11739664524793625 action_loss -0.2042262852191925 dist_entropy 1.94243323802948\n",
      "Updates 945, num timesteps 804100, FPS 271 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.07379595935344696 action_loss -0.0974237397313118 dist_entropy 1.9323649406433105\n",
      "Updates 950, num timesteps 808350, FPS 271 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.12866193056106567 action_loss 0.0806107372045517 dist_entropy 1.965833306312561\n",
      "Updates 955, num timesteps 812600, FPS 271 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.08661969006061554 action_loss -0.09629412740468979 dist_entropy 1.9264386892318726\n",
      "Updates 960, num timesteps 816850, FPS 271 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.10499504208564758 action_loss 0.17565196752548218 dist_entropy 1.9336140155792236\n",
      "Updates 965, num timesteps 821100, FPS 271 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.06400883942842484 action_loss -0.15336903929710388 dist_entropy 1.928560733795166\n",
      "Updates 970, num timesteps 825350, FPS 271 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.05678178369998932 action_loss 0.11241465061903 dist_entropy 1.9216914176940918\n",
      "Updates 975, num timesteps 829600, FPS 271 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.07358086109161377 action_loss 0.05545060709118843 dist_entropy 1.9301776885986328\n",
      "Updates 980, num timesteps 833850, FPS 271 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.07986986637115479 action_loss -0.12192752957344055 dist_entropy 1.9384982585906982\n",
      "Updates 985, num timesteps 838100, FPS 270 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.06293704360723495 action_loss -0.06452297419309616 dist_entropy 1.942000389099121\n",
      "Updates 990, num timesteps 842350, FPS 270 \n",
      " Last 10 training episodes: mean/median reward -0.3/-0.2, min/max reward -1.0/-0.2\n",
      "\n",
      "value_loss 0.07777151465415955 action_loss -0.2724778354167938 dist_entropy 1.9198740720748901\n",
      "Updates 995, num timesteps 846600, FPS 270 \n",
      " Last 10 training episodes: mean/median reward -0.2/-0.2, min/max reward -0.2/-0.2\n",
      "\n",
      "value_loss 0.07942337542772293 action_loss -0.15041162073612213 dist_entropy 1.9431875944137573\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import GridWorld_env\n",
    "def main():\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    n_actions = 8\n",
    "    env = gym.make(\"GridWorld_env/GridWorld\", dimension_size=4, path=\"targets\")\n",
    "    env.reset()\n",
    "\n",
    "\n",
    "    actor_critic = Policy(\n",
    "        obs_shape=(3, ENV_DIM, ENV_DIM, ENV_DIM),\n",
    "        action_space=n_actions,\n",
    "        base=CNNBase,\n",
    "        )\n",
    "    actor_critic.to(device)\n",
    "    \n",
    "    \n",
    "    agent = A2C(\n",
    "        actor_critic=actor_critic,\n",
    "        value_loss_coef=VALUE_COEFF,\n",
    "        entropy_coef=ENTROPY_COEFF,\n",
    "        lr=LR,\n",
    "        eps=RMS_EPSILON,\n",
    "        alpha=RMS_ALPHA,\n",
    "        max_grad_norm=5\n",
    "    )\n",
    "    \"\"\"\n",
    "    if args.gail:\n",
    "        assert len(envs.observation_space.shape) == 1\n",
    "        discr = gail.Discriminator(\n",
    "            envs.observation_space.shape[0] + envs.action_space.shape[0], 100,\n",
    "            device)\n",
    "        file_name = os.path.join(\n",
    "            args.gail_experts_dir, \"trajs_{}.pt\".format(\n",
    "                args.env_name.split('-')[0].lower()))\n",
    "        \n",
    "        expert_dataset = gail.ExpertDataset(\n",
    "            file_name, num_trajectories=4, subsample_frequency=20)\n",
    "        drop_last = len(expert_dataset) > args.gail_batch_size\n",
    "        gail_train_loader = torch.utils.data.DataLoader(\n",
    "            dataset=expert_dataset,\n",
    "            batch_size=args.gail_batch_size,\n",
    "            shuffle=True,\n",
    "            drop_last=drop_last)\"\"\"\n",
    "\n",
    "    rollouts = RolloutStorage(num_steps=NUM_STEP, num_processes=NUM_PROCESSES,\n",
    "                              obs_shape=(3, ENV_DIM, ENV_DIM, ENV_DIM), action_space=NUM_ACTION,\n",
    "                              recurrent_hidden_state_size=RECURRENT_HIDDEN_SIZE)\n",
    "\n",
    "    obs = env.get_obs()\n",
    "    obs = torch.from_numpy(obs).float()\n",
    "    rollouts.obs[0].copy_(obs)\n",
    "    rollouts.to(device)\n",
    "\n",
    "    episode_rewards = deque(maxlen=10)\n",
    "\n",
    "    start = time.time()\n",
    "    num_updates = int(\n",
    "        NUM_STEP_TOTAL) // NUM_STEP // NUM_PROCESSES\n",
    "    for j in range(num_updates):\n",
    "        env.reset()\n",
    "        \"\"\"if USE_LR_DECAY:\n",
    "            # decrease learning rate linearly\n",
    "            utils.update_linear_schedule(\n",
    "                agent.optimizer, j, num_updates,\n",
    "                agent.optimizer.lr if args.algo == \"acktr\" else args.lr)\"\"\"\n",
    "\n",
    "        for step in range(NUM_STEP):\n",
    "            # Sample actions\n",
    "            with torch.no_grad():\n",
    "                value, action, action_log_prob, recurrent_hidden_states = actor_critic.act(\n",
    "                    rollouts.obs[step], rollouts.recurrent_hidden_states[step],\n",
    "                    rollouts.masks[step])\n",
    "\n",
    "            # Obser reward and next obs\n",
    "            obs, reward, done, truncated, infos = env.step(action)\n",
    "            obs = torch.from_numpy(obs).float()\n",
    "            \n",
    "            done = [done]\n",
    "            episode_rewards.append(reward)\n",
    "\n",
    "            # If done then clean the history of observations.\n",
    "            masks = torch.FloatTensor(\n",
    "                [[0.0] if done_ else [1.0] for done_ in done])\n",
    "            \"\"\"bad_masks = torch.FloatTensor(\n",
    "                [[0.0] if 'bad_transition' in info.keys() else [1.0]\n",
    "                 for info in infos])\"\"\"\n",
    "                 \n",
    "            bad_masks = torch.FloatTensor(\n",
    "                [[0.0] if done_ else [1.0] for done_ in done])\n",
    "            \n",
    "            rollouts.insert(obs, recurrent_hidden_states, action,\n",
    "                            action_log_prob, value, reward, masks, bad_masks)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_value = actor_critic.get_value(\n",
    "                rollouts.obs[-1], rollouts.recurrent_hidden_states[-1],\n",
    "                rollouts.masks[-1]).detach()\n",
    "\n",
    "        \"\"\"if args.gail:\n",
    "            if j >= 10:\n",
    "                env.venv.eval()\n",
    "\n",
    "            gail_epoch = args.gail_epoch\n",
    "            if j < 10:\n",
    "                gail_epoch = 100  # Warm up\n",
    "            for _ in range(gail_epoch):\n",
    "                discr.update(gail_train_loader, rollouts,\n",
    "                             utils.get_vec_normalize(env)._obfilt)\n",
    "\n",
    "            for step in range(NUM_STEP):\n",
    "                rollouts.rewards[step] = discr.predict_reward(\n",
    "                    rollouts.obs[step], rollouts.actions[step], args.gamma,\n",
    "                    rollouts.masks[step])\"\"\"\n",
    "\n",
    "        rollouts.compute_returns(next_value, use_gae=False, gamma=GAMMA,\n",
    "                                 gae_lambda=0, use_proper_time_limits=False)\n",
    "\n",
    "        value_loss, action_loss, dist_entropy = agent.update(rollouts, j)\n",
    "\n",
    "        rollouts.after_update()\n",
    "\n",
    "        # save for every interval-th episode or for the last epoch\n",
    "        if (j % 200 == 0\n",
    "                or j == num_updates - 1):\n",
    "\n",
    "            torch.save(#[\n",
    "                actor_critic,\n",
    "                #getattr(utils.get_vec_normalize(env), 'obs_rms', None)],\n",
    "            \"A2C_checkpoint_\" + str(j) + \".pt\")\n",
    "\n",
    "        if j % 5 == 0 and len(episode_rewards) > 1:\n",
    "            total_num_steps = (j + 1) * NUM_PROCESSES * NUM_STEP\n",
    "            end = time.time()\n",
    "            print(\n",
    "                \"Updates {}, num timesteps {}, FPS {} \\n Last {} training episodes: mean/median reward {:.1f}/{:.1f}, min/max reward {:.1f}/{:.1f}\\n\"\n",
    "                .format(j, total_num_steps,\n",
    "                        int(total_num_steps / (end - start)),\n",
    "                        len(episode_rewards), np.mean(episode_rewards),\n",
    "                        np.median(episode_rewards), np.min(episode_rewards),\n",
    "                        np.max(episode_rewards)))\n",
    "            print(\"value_loss\", value_loss, \"action_loss\", action_loss, \"dist_entropy\", dist_entropy)\n",
    "            #env.render()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\tutha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\ImageFile.py:518\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[0;32m    517\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 518\u001b[0m     fh \u001b[38;5;241m=\u001b[39m \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfileno\u001b[49m()\n\u001b[0;32m    519\u001b[0m     fp\u001b[38;5;241m.\u001b[39mflush()\n",
      "\u001b[1;31mAttributeError\u001b[0m: '_idat' object has no attribute 'fileno'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Obser reward and next obs\u001b[39;00m\n\u001b[0;32m     23\u001b[0m obs, reward, done, truncated, infos \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m---> 24\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m display\u001b[38;5;241m.\u001b[39mclear_output(wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     26\u001b[0m obs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(obs)\u001b[38;5;241m.\u001b[39mfloat()\n",
      "File \u001b[1;32mc:\\Users\\tutha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:70\u001b[0m, in \u001b[0;36mOrderEnforcing.render\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_render_order_enforcing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\n\u001b[0;32m     67\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call `env.render()` before calling `env.reset()`, if this is a intended action, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     69\u001b[0m     )\n\u001b[1;32m---> 70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tutha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:67\u001b[0m, in \u001b[0;36mPassiveEnvChecker.render\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_render_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\tutha\\desktop\\cmput469\\deep-reinforcement-learning-agent-for-construction-project-execution\\gridworld-env\\GridWorld_env\\envs\\grid_world.py:222\u001b[0m, in \u001b[0;36mGridWorldEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    219\u001b[0m ax \u001b[38;5;241m=\u001b[39m fig\u001b[38;5;241m.\u001b[39madd_subplot(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, projection\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3d\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    220\u001b[0m ax\u001b[38;5;241m.\u001b[39mvoxels(target_render, facecolors\u001b[38;5;241m=\u001b[39mcolors, edgecolor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 222\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tutha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\pyplot.py:446\u001b[0m, in \u001b[0;36mshow\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;124;03mDisplay all open figures.\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;124;03mexplicitly there.\u001b[39;00m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    445\u001b[0m _warn_if_gui_out_of_main_thread()\n\u001b[1;32m--> 446\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_backend_mod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\matplotlib_inline\\backend_inline.py:90\u001b[0m, in \u001b[0;36mshow\u001b[1;34m(close, block)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m figure_manager \u001b[38;5;129;01min\u001b[39;00m Gcf\u001b[38;5;241m.\u001b[39mget_all_fig_managers():\n\u001b[1;32m---> 90\u001b[0m         \u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfigure_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fetch_figure_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfigure_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     show\u001b[38;5;241m.\u001b[39m_to_draw \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\display_functions.py:298\u001b[0m, in \u001b[0;36mdisplay\u001b[1;34m(include, exclude, metadata, transient, display_id, raw, clear, *objs, **kwargs)\u001b[0m\n\u001b[0;32m    296\u001b[0m     publish_display_data(data\u001b[38;5;241m=\u001b[39mobj, metadata\u001b[38;5;241m=\u001b[39mmetadata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 298\u001b[0m     format_dict, md_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m format_dict:\n\u001b[0;32m    300\u001b[0m         \u001b[38;5;66;03m# nothing to display (e.g. _ipython_display_ took over)\u001b[39;00m\n\u001b[0;32m    301\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\formatters.py:177\u001b[0m, in \u001b[0;36mDisplayFormatter.format\u001b[1;34m(self, obj, include, exclude)\u001b[0m\n\u001b[0;32m    175\u001b[0m md \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 177\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;66;03m# FIXME: log the exception\u001b[39;00m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[0;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcaller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mextras\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\formatters.py:221\u001b[0m, in \u001b[0;36mcatch_format_error\u001b[1;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"show traceback on failed format call\"\"\"\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 221\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;66;03m# don't warn on NotImplementedErrors\u001b[39;00m\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_return(\u001b[38;5;28;01mNone\u001b[39;00m, args[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\formatters.py:338\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 338\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprinter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;66;03m# Finally look for special method names\u001b[39;00m\n\u001b[0;32m    340\u001b[0m method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\pylabtools.py:152\u001b[0m, in \u001b[0;36mprint_figure\u001b[1;34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_bases\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FigureCanvasBase\n\u001b[0;32m    150\u001b[0m     FigureCanvasBase(fig)\n\u001b[1;32m--> 152\u001b[0m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_figure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbytes_io\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    153\u001b[0m data \u001b[38;5;241m=\u001b[39m bytes_io\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fmt \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\tutha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\backend_bases.py:2366\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[1;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[0;32m   2362\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2363\u001b[0m     \u001b[38;5;66;03m# _get_renderer may change the figure dpi (as vector formats\u001b[39;00m\n\u001b[0;32m   2364\u001b[0m     \u001b[38;5;66;03m# force the figure dpi to 72), so we need to set it again here.\u001b[39;00m\n\u001b[0;32m   2365\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m cbook\u001b[38;5;241m.\u001b[39m_setattr_cm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure, dpi\u001b[38;5;241m=\u001b[39mdpi):\n\u001b[1;32m-> 2366\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mprint_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2367\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2368\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfacecolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfacecolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2369\u001b[0m \u001b[43m            \u001b[49m\u001b[43medgecolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medgecolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2370\u001b[0m \u001b[43m            \u001b[49m\u001b[43morientation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morientation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2371\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbbox_inches_restore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_bbox_inches_restore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2372\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2373\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   2374\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bbox_inches \u001b[38;5;129;01mand\u001b[39;00m restore_bbox:\n",
      "File \u001b[1;32mc:\\Users\\tutha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\backend_bases.py:2232\u001b[0m, in \u001b[0;36mFigureCanvasBase._switch_canvas_and_return_print_method.<locals>.<lambda>\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   2228\u001b[0m     optional_kws \u001b[38;5;241m=\u001b[39m {  \u001b[38;5;66;03m# Passed by print_figure for other renderers.\u001b[39;00m\n\u001b[0;32m   2229\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdpi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medgecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morientation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2230\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbbox_inches_restore\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m   2231\u001b[0m     skip \u001b[38;5;241m=\u001b[39m optional_kws \u001b[38;5;241m-\u001b[39m {\u001b[38;5;241m*\u001b[39minspect\u001b[38;5;241m.\u001b[39msignature(meth)\u001b[38;5;241m.\u001b[39mparameters}\n\u001b[1;32m-> 2232\u001b[0m     print_method \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mwraps(meth)(\u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2233\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   2234\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Let third-parties do as they see fit.\u001b[39;00m\n\u001b[0;32m   2235\u001b[0m     print_method \u001b[38;5;241m=\u001b[39m meth\n",
      "File \u001b[1;32mc:\\Users\\tutha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\backends\\backend_agg.py:509\u001b[0m, in \u001b[0;36mFigureCanvasAgg.print_png\u001b[1;34m(self, filename_or_obj, metadata, pil_kwargs)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprint_png\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename_or_obj, \u001b[38;5;241m*\u001b[39m, metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, pil_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    463\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    464\u001b[0m \u001b[38;5;124;03m    Write the figure to a PNG file.\u001b[39;00m\n\u001b[0;32m    465\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;124;03m        *metadata*, including the default 'Software' key.\u001b[39;00m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 509\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_print_pil\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpng\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpil_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tutha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\backends\\backend_agg.py:458\u001b[0m, in \u001b[0;36mFigureCanvasAgg._print_pil\u001b[1;34m(self, filename_or_obj, fmt, pil_kwargs, metadata)\u001b[0m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;124;03mDraw the canvas, then save it using `.image.imsave` (to which\u001b[39;00m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;124;03m*pil_kwargs* and *metadata* are forwarded).\u001b[39;00m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    457\u001b[0m FigureCanvasAgg\u001b[38;5;241m.\u001b[39mdraw(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 458\u001b[0m \u001b[43mmpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimsave\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuffer_rgba\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfmt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morigin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mupper\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdpi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdpi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpil_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpil_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tutha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\image.py:1689\u001b[0m, in \u001b[0;36mimsave\u001b[1;34m(fname, arr, vmin, vmax, cmap, format, origin, dpi, metadata, pil_kwargs)\u001b[0m\n\u001b[0;32m   1687\u001b[0m pil_kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mformat\u001b[39m)\n\u001b[0;32m   1688\u001b[0m pil_kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdpi\u001b[39m\u001b[38;5;124m\"\u001b[39m, (dpi, dpi))\n\u001b[1;32m-> 1689\u001b[0m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpil_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tutha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\Image.py:2431\u001b[0m, in \u001b[0;36mImage.save\u001b[1;34m(self, fp, format, **params)\u001b[0m\n\u001b[0;32m   2428\u001b[0m         fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw+b\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2430\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2431\u001b[0m     \u001b[43msave_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2432\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   2433\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m open_fp:\n",
      "File \u001b[1;32mc:\\Users\\tutha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\PngImagePlugin.py:1420\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(im, fp, filename, chunk, save_all)\u001b[0m\n\u001b[0;32m   1418\u001b[0m     _write_multiple_frames(im, fp, chunk, rawmode, default_image, append_images)\n\u001b[0;32m   1419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1420\u001b[0m     \u001b[43mImageFile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_idat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mzip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrawmode\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info:\n\u001b[0;32m   1423\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m info_chunk \u001b[38;5;129;01min\u001b[39;00m info\u001b[38;5;241m.\u001b[39mchunks:\n",
      "File \u001b[1;32mc:\\Users\\tutha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\ImageFile.py:522\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[0;32m    520\u001b[0m     _encode_tile(im, fp, tile, bufsize, fh)\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, io\u001b[38;5;241m.\u001b[39mUnsupportedOperation) \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m--> 522\u001b[0m     \u001b[43m_encode_tile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflush\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    524\u001b[0m     fp\u001b[38;5;241m.\u001b[39mflush()\n",
      "File \u001b[1;32mc:\\Users\\tutha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\ImageFile.py:541\u001b[0m, in \u001b[0;36m_encode_tile\u001b[1;34m(im, fp, tile, bufsize, fh, exc)\u001b[0m\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exc:\n\u001b[0;32m    539\u001b[0m     \u001b[38;5;66;03m# compress to Python file-compatible object\u001b[39;00m\n\u001b[0;32m    540\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 541\u001b[0m         l, s, d \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    542\u001b[0m         fp\u001b[38;5;241m.\u001b[39mwrite(d)\n\u001b[0;32m    543\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m s:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make(\"GridWorld_env/GridWorld\", dimension_size=4, path=\"targets\")\n",
    "from IPython import display\n",
    "env.reset()\n",
    "device = 'cuda'\n",
    "actor_critic = torch.load(\"A2C_checkpoint_999.pt\")\n",
    "rollouts = RolloutStorage(num_steps=NUM_STEP, num_processes=NUM_PROCESSES,\n",
    "                            obs_shape=(3, ENV_DIM, ENV_DIM, ENV_DIM), action_space=NUM_ACTION,\n",
    "                            recurrent_hidden_state_size=RECURRENT_HIDDEN_SIZE)\n",
    "\n",
    "obs = env.get_obs()\n",
    "obs = torch.from_numpy(obs).float()\n",
    "rollouts.obs[0].copy_(obs)\n",
    "rollouts.to(device)\n",
    "\n",
    "for step in range(NUM_STEP):\n",
    "        # Sample actions\n",
    "        with torch.no_grad():\n",
    "            value, action, action_log_prob, recurrent_hidden_states = actor_critic.act(\n",
    "                rollouts.obs[step], rollouts.recurrent_hidden_states[step],\n",
    "                rollouts.masks[step])\n",
    "\n",
    "        # Obser reward and next obs\n",
    "        obs, reward, done, truncated, infos = env.step(action)\n",
    "        env.render()\n",
    "        display.clear_output(wait=True)\n",
    "        obs = torch.from_numpy(obs).float()\n",
    "        \n",
    "        done = [done]\n",
    "\n",
    "        # If done then clean the history of observations.\n",
    "        masks = torch.FloatTensor(\n",
    "            [[0.0] if done_ else [1.0] for done_ in done])\n",
    "        \"\"\"bad_masks = torch.FloatTensor(\n",
    "            [[0.0] if 'bad_transition' in info.keys() else [1.0]\n",
    "                for info in infos])\"\"\"\n",
    "                \n",
    "        bad_masks = torch.FloatTensor(\n",
    "            [[0.0] if done_ else [1.0] for done_ in done])\n",
    "        \n",
    "        rollouts.insert(obs, recurrent_hidden_states, action,\n",
    "                        action_log_prob, value, reward, masks, bad_masks)\n",
    "\n",
    "with torch.no_grad():\n",
    "    next_value = actor_critic.get_value(\n",
    "        rollouts.obs[-1], rollouts.recurrent_hidden_states[-1],\n",
    "        rollouts.masks[-1]).detach()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m count():\n\u001b[0;32m      4\u001b[0m     action, _, _, _, _ \u001b[38;5;241m=\u001b[39m net(torch\u001b[38;5;241m.\u001b[39mfrom_numpy(env\u001b[38;5;241m.\u001b[39mget_obs())\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mcuda())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "env.reset()\n",
    "for i in count():\n",
    "    \n",
    "    action, _, _, _, _ = net(torch.from_numpy(env.get_obs()).unsqueeze(0).float().cuda())\n",
    "    env.step(action.cpu().numpy())\n",
    "    env.render()\n",
    "    display.clear_output(wait=True)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
