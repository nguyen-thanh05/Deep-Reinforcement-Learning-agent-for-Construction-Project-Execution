{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from replay_buffer import ReplayBuffer\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "class A2C():\n",
    "    def __init__(self,\n",
    "                 actor_critic,\n",
    "                 value_loss_coef,\n",
    "                 entropy_coef,\n",
    "                 lr=None,\n",
    "                 eps=None,\n",
    "                 alpha=None,\n",
    "                 max_grad_norm=None):\n",
    "\n",
    "        self.summary_writer = SummaryWriter()\n",
    "        self.actor_critic = actor_critic\n",
    "        \n",
    "\n",
    "        self.value_loss_coef = value_loss_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "\n",
    "        \n",
    "        self.optimizer = optim.RMSprop(\n",
    "            actor_critic.parameters(), lr, weight_decay=1e-7)\n",
    "\n",
    "    def update(self, rollouts, episode_num):\n",
    "        obs_shape = rollouts.obs.size()[2:]\n",
    "        action_shape = rollouts.actions.size()[-1]\n",
    "        num_steps, num_processes, _ = rollouts.rewards.size()\n",
    "\n",
    "        values, action_log_probs, dist_entropy, _ = self.actor_critic.evaluate_actions(\n",
    "            rollouts.obs[:-1].view(-1, *obs_shape),\n",
    "            rollouts.recurrent_hidden_states[0].view(\n",
    "                -1, self.actor_critic.recurrent_hidden_state_size),\n",
    "            rollouts.masks[:-1].view(-1, 1),\n",
    "            rollouts.actions.view(-1, action_shape))\n",
    "        #print(rollouts.obs[:-1].view(-1, *obs_shape).shape, rollouts.recurrent_hidden_states[0].view(\n",
    "        #        -1, self.actor_critic.recurrent_hidden_state_size).shape, rollouts.masks[:-1].view(-1, 1).shape, rollouts.actions.view(-1, action_shape).shape)\n",
    "        values = values.view(num_steps, num_processes, 1)\n",
    "        action_log_probs = action_log_probs.view(num_steps, num_processes, 1)\n",
    "\n",
    "        advantages = rollouts.returns[:-1] - values\n",
    "        value_loss = advantages.pow(2).mean()\n",
    "\n",
    "        action_loss = (advantages.detach() * action_log_probs).mean()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        (value_loss * self.value_loss_coef - action_loss - dist_entropy * self.entropy_coef).backward()\n",
    "\n",
    "        self.summary_writer.add_scalar('value_loss', value_loss.item(), episode_num)\n",
    "        self.summary_writer.add_scalar('policy_loss', action_loss.item(), episode_num)\n",
    "        self.summary_writer.add_scalar('dist_entropy', dist_entropy.item(), episode_num)\n",
    "        nn.utils.clip_grad_norm_(self.actor_critic.parameters(),\n",
    "                                     self.max_grad_norm)\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return value_loss.item(), action_loss.item(), dist_entropy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO():\n",
    "    def __init__(self,\n",
    "                 actor_critic,\n",
    "                 clip_param,\n",
    "                 ppo_epoch,\n",
    "                 num_mini_batch,\n",
    "                 value_loss_coef,\n",
    "                 entropy_coef,\n",
    "                 lr=None,\n",
    "                 eps=None,\n",
    "                 max_grad_norm=None,\n",
    "                 use_clipped_value_loss=True):\n",
    "\n",
    "        self.actor_critic = actor_critic\n",
    "\n",
    "        self.clip_param = clip_param\n",
    "        self.ppo_epoch = ppo_epoch\n",
    "        self.num_mini_batch = num_mini_batch\n",
    "\n",
    "        self.value_loss_coef = value_loss_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.use_clipped_value_loss = use_clipped_value_loss\n",
    "\n",
    "        self.optimizer = optim.Adam(actor_critic.parameters(), lr=lr, eps=eps)\n",
    "        self.writer = SummaryWriter()\n",
    "        \n",
    "\n",
    "    def update(self, rollouts, j, entropy_coefficent):\n",
    "        advantages = rollouts.returns[:-1] - rollouts.value_preds[:-1]\n",
    "        advantages = (advantages - advantages.mean()) / (\n",
    "            advantages.std() + 1e-5)\n",
    "\n",
    "        value_loss_epoch = 0\n",
    "        action_loss_epoch = 0\n",
    "        dist_entropy_epoch = 0\n",
    "\n",
    "        for e in range(self.ppo_epoch):\n",
    "            if self.actor_critic.is_recurrent:\n",
    "                data_generator = rollouts.recurrent_generator(\n",
    "                    advantages, self.num_mini_batch)\n",
    "            else:\n",
    "                data_generator = rollouts.feed_forward_generator(\n",
    "                    advantages, self.num_mini_batch)\n",
    "\n",
    "            for sample in data_generator:\n",
    "                obs_batch, recurrent_hidden_states_batch, actions_batch, \\\n",
    "                   value_preds_batch, return_batch, masks_batch, old_action_log_probs_batch, \\\n",
    "                        adv_targ = sample\n",
    "\n",
    "                # Reshape to do in a single forward pass for all steps\n",
    "                values, action_log_probs, dist_entropy, _ = self.actor_critic.evaluate_actions(\n",
    "                    obs_batch, recurrent_hidden_states_batch, masks_batch,\n",
    "                    actions_batch)\n",
    "\n",
    "                ratio = torch.exp(action_log_probs -\n",
    "                                  old_action_log_probs_batch)\n",
    "                surr1 = ratio * adv_targ\n",
    "                surr2 = torch.clamp(ratio, 1.0 - self.clip_param,\n",
    "                                    1.0 + self.clip_param) * adv_targ\n",
    "                action_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "                if self.use_clipped_value_loss:\n",
    "                    value_pred_clipped = value_preds_batch + \\\n",
    "                        (values - value_preds_batch).clamp(-self.clip_param, self.clip_param)\n",
    "                    value_losses = (values - return_batch).pow(2)\n",
    "                    value_losses_clipped = (\n",
    "                        value_pred_clipped - return_batch).pow(2)\n",
    "                    value_loss = 0.5 * torch.max(value_losses,\n",
    "                                                 value_losses_clipped).mean()\n",
    "                else:\n",
    "                    value_loss = 0.5 * (return_batch - values).pow(2).mean()\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                (value_loss * self.value_loss_coef + action_loss -\n",
    "                 dist_entropy * entropy_coefficent).backward()\n",
    "                nn.utils.clip_grad_norm_(self.actor_critic.parameters(),\n",
    "                                         self.max_grad_norm)\n",
    "                self.optimizer.step()\n",
    "\n",
    "                value_loss_epoch += value_loss.item()\n",
    "                action_loss_epoch += action_loss.item()\n",
    "                dist_entropy_epoch += dist_entropy.item()\n",
    "\n",
    "        num_updates = self.ppo_epoch * self.num_mini_batch\n",
    "\n",
    "        value_loss_epoch /= num_updates\n",
    "        action_loss_epoch /= num_updates\n",
    "        dist_entropy_epoch /= num_updates\n",
    "\n",
    "        self.writer.add_scalar('value_loss', value_loss_epoch, j)\n",
    "        self.writer.add_scalar('policy_loss', action_loss_epoch, j)\n",
    "        self.writer.add_scalar('dist_entropy', dist_entropy_epoch, j)\n",
    "        \n",
    "        return value_loss_epoch, action_loss_epoch, dist_entropy_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
    "\n",
    "\n",
    "def _flatten_helper(T, N, _tensor):\n",
    "    return _tensor.view(T * N, *_tensor.size()[2:])\n",
    "\n",
    "\n",
    "class RolloutStorage(object):\n",
    "    def __init__(self, num_steps, num_processes, obs_shape, action_space,\n",
    "                 recurrent_hidden_state_size):\n",
    "        self.obs = torch.zeros(num_steps + 1, num_processes, *obs_shape)\n",
    "        self.recurrent_hidden_states = torch.zeros(\n",
    "            num_steps + 1, num_processes, recurrent_hidden_state_size)\n",
    "        self.rewards = torch.zeros(num_steps, num_processes, 1)\n",
    "        self.value_preds = torch.zeros(num_steps + 1, num_processes, 1)\n",
    "        self.returns = torch.zeros(num_steps + 1, num_processes, 1)\n",
    "        self.action_log_probs = torch.zeros(num_steps, num_processes, 1)\n",
    "        #if action_space.__class__.__name__ == 'Discrete':\n",
    "        action_shape = 1\n",
    "        #else:\n",
    "        #    action_shape = action_space.shape[0]\n",
    "        self.actions = torch.zeros(num_steps, num_processes, action_shape)\n",
    "        if action_space.__class__.__name__ == 'Discrete':\n",
    "            self.actions = self.actions.long()\n",
    "        self.masks = torch.ones(num_steps + 1, num_processes, 1)\n",
    "\n",
    "        # Masks that indicate whether it's a true terminal state\n",
    "        # or time limit end state\n",
    "        self.bad_masks = torch.ones(num_steps + 1, num_processes, 1)\n",
    "\n",
    "        self.num_steps = num_steps\n",
    "        self.step = 0\n",
    "\n",
    "    def to(self, device):\n",
    "        self.obs = self.obs.to(device)\n",
    "        self.recurrent_hidden_states = self.recurrent_hidden_states.to(device)\n",
    "        self.rewards = self.rewards.to(device)\n",
    "        self.value_preds = self.value_preds.to(device)\n",
    "        self.returns = self.returns.to(device)\n",
    "        self.action_log_probs = self.action_log_probs.to(device)\n",
    "        self.actions = self.actions.to(device)\n",
    "        self.masks = self.masks.to(device)\n",
    "        self.bad_masks = self.bad_masks.to(device)\n",
    "\n",
    "    def insert(self, obs, recurrent_hidden_states, actions, action_log_probs,\n",
    "               value_preds, rewards, masks, bad_masks):\n",
    "        self.obs[self.step + 1].copy_(obs)\n",
    "        self.recurrent_hidden_states[self.step +\n",
    "                                     1].copy_(recurrent_hidden_states)\n",
    "        self.actions[self.step].copy_(actions)\n",
    "        self.action_log_probs[self.step].copy_(action_log_probs)\n",
    "        self.value_preds[self.step].copy_(value_preds)\n",
    "        self.rewards[self.step].copy_(rewards)\n",
    "        self.masks[self.step + 1].copy_(masks)\n",
    "        self.bad_masks[self.step + 1].copy_(bad_masks)\n",
    "\n",
    "        self.step = (self.step + 1) % self.num_steps\n",
    "\n",
    "    def after_update(self):\n",
    "        self.obs[0].copy_(self.obs[-1])\n",
    "        self.recurrent_hidden_states[0].copy_(self.recurrent_hidden_states[-1])\n",
    "        self.masks[0].copy_(self.masks[-1])\n",
    "        self.bad_masks[0].copy_(self.bad_masks[-1])\n",
    "\n",
    "    def compute_returns(self,\n",
    "                        next_value,\n",
    "                        use_gae,\n",
    "                        gamma,\n",
    "                        gae_lambda,\n",
    "                        use_proper_time_limits=True):\n",
    "        if use_proper_time_limits:\n",
    "            if use_gae:\n",
    "                self.value_preds[-1] = next_value\n",
    "                gae = 0\n",
    "                for step in reversed(range(self.rewards.size(0))):\n",
    "                    delta = self.rewards[step] + gamma * self.value_preds[\n",
    "                        step + 1] * self.masks[step +\n",
    "                                               1] - self.value_preds[step]\n",
    "                    gae = delta + gamma * gae_lambda * self.masks[step +\n",
    "                                                                  1] * gae\n",
    "                    gae = gae * self.bad_masks[step + 1]\n",
    "                    self.returns[step] = gae + self.value_preds[step]\n",
    "            else:\n",
    "                self.returns[-1] = next_value\n",
    "                for step in reversed(range(self.rewards.size(0))):\n",
    "                    self.returns[step] = (self.returns[step + 1] * \\\n",
    "                        gamma * self.masks[step + 1] + self.rewards[step]) * self.bad_masks[step + 1] \\\n",
    "                        + (1 - self.bad_masks[step + 1]) * self.value_preds[step]\n",
    "        else:\n",
    "            if use_gae:\n",
    "                self.value_preds[-1] = next_value\n",
    "                gae = 0\n",
    "                for step in reversed(range(self.rewards.size(0))):\n",
    "                    delta = self.rewards[step] + gamma * self.value_preds[\n",
    "                        step + 1] * self.masks[step +\n",
    "                                               1] - self.value_preds[step]\n",
    "                    gae = delta + gamma * gae_lambda * self.masks[step +\n",
    "                                                                  1] * gae\n",
    "                    self.returns[step] = gae + self.value_preds[step]\n",
    "            else:\n",
    "                self.returns[-1] = next_value\n",
    "                for step in reversed(range(self.rewards.size(0))):\n",
    "                    self.returns[step] = self.returns[step + 1] * \\\n",
    "                        gamma * self.masks[step + 1] + self.rewards[step]\n",
    "\n",
    "\n",
    "    def feed_forward_generator(self,\n",
    "                               advantages,\n",
    "                               num_mini_batch=None,\n",
    "                               mini_batch_size=None):\n",
    "        num_steps, num_processes = self.rewards.size()[0:2]\n",
    "        batch_size = num_processes * num_steps\n",
    "\n",
    "        if mini_batch_size is None:\n",
    "            assert batch_size >= num_mini_batch, (\n",
    "                \"PPO requires the number of processes ({}) \"\n",
    "                \"* number of steps ({}) = {} \"\n",
    "                \"to be greater than or equal to the number of PPO mini batches ({}).\"\n",
    "                \"\".format(num_processes, num_steps, num_processes * num_steps,\n",
    "                          num_mini_batch))\n",
    "            mini_batch_size = batch_size // num_mini_batch\n",
    "        sampler = BatchSampler(\n",
    "            SubsetRandomSampler(range(batch_size)),\n",
    "            mini_batch_size,\n",
    "            drop_last=True)\n",
    "        for indices in sampler:\n",
    "            obs_batch = self.obs[:-1].view(-1, *self.obs.size()[2:])[indices]\n",
    "            recurrent_hidden_states_batch = self.recurrent_hidden_states[:-1].view(\n",
    "                -1, self.recurrent_hidden_states.size(-1))[indices]\n",
    "            actions_batch = self.actions.view(-1,\n",
    "                                              self.actions.size(-1))[indices]\n",
    "            value_preds_batch = self.value_preds[:-1].view(-1, 1)[indices]\n",
    "            return_batch = self.returns[:-1].view(-1, 1)[indices]\n",
    "            masks_batch = self.masks[:-1].view(-1, 1)[indices]\n",
    "            old_action_log_probs_batch = self.action_log_probs.view(-1,\n",
    "                                                                    1)[indices]\n",
    "            if advantages is None:\n",
    "                adv_targ = None\n",
    "            else:\n",
    "                adv_targ = advantages.view(-1, 1)[indices]\n",
    "\n",
    "            yield obs_batch, recurrent_hidden_states_batch, actions_batch, \\\n",
    "                value_preds_batch, return_batch, masks_batch, old_action_log_probs_batch, adv_targ\n",
    "\n",
    "    def recurrent_generator(self, advantages, num_mini_batch):\n",
    "        num_processes = self.rewards.size(1)\n",
    "        assert num_processes >= num_mini_batch, (\n",
    "            \"PPO requires the number of processes ({}) \"\n",
    "            \"to be greater than or equal to the number of \"\n",
    "            \"PPO mini batches ({}).\".format(num_processes, num_mini_batch))\n",
    "        num_envs_per_batch = num_processes // num_mini_batch\n",
    "        perm = torch.randperm(num_processes)\n",
    "        for start_ind in range(0, num_processes, num_envs_per_batch):\n",
    "            obs_batch = []\n",
    "            recurrent_hidden_states_batch = []\n",
    "            actions_batch = []\n",
    "            value_preds_batch = []\n",
    "            return_batch = []\n",
    "            masks_batch = []\n",
    "            old_action_log_probs_batch = []\n",
    "            adv_targ = []\n",
    "\n",
    "            for offset in range(num_envs_per_batch):\n",
    "                ind = perm[start_ind + offset]\n",
    "                obs_batch.append(self.obs[:-1, ind])\n",
    "                recurrent_hidden_states_batch.append(\n",
    "                    self.recurrent_hidden_states[0:1, ind])\n",
    "                actions_batch.append(self.actions[:, ind])\n",
    "                value_preds_batch.append(self.value_preds[:-1, ind])\n",
    "                return_batch.append(self.returns[:-1, ind])\n",
    "                masks_batch.append(self.masks[:-1, ind])\n",
    "                old_action_log_probs_batch.append(\n",
    "                    self.action_log_probs[:, ind])\n",
    "                adv_targ.append(advantages[:, ind])\n",
    "\n",
    "            T, N = self.num_steps, num_envs_per_batch\n",
    "            # These are all tensors of size (T, N, -1)\n",
    "            obs_batch = torch.stack(obs_batch, 1)\n",
    "            actions_batch = torch.stack(actions_batch, 1)\n",
    "            value_preds_batch = torch.stack(value_preds_batch, 1)\n",
    "            return_batch = torch.stack(return_batch, 1)\n",
    "            masks_batch = torch.stack(masks_batch, 1)\n",
    "            old_action_log_probs_batch = torch.stack(\n",
    "                old_action_log_probs_batch, 1)\n",
    "            adv_targ = torch.stack(adv_targ, 1)\n",
    "\n",
    "            # States is just a (N, -1) tensor\n",
    "            recurrent_hidden_states_batch = torch.stack(\n",
    "                recurrent_hidden_states_batch, 1).view(N, -1)\n",
    "\n",
    "            # Flatten the (T, N, ...) tensors to (T * N, ...)\n",
    "            obs_batch = _flatten_helper(T, N, obs_batch)\n",
    "            actions_batch = _flatten_helper(T, N, actions_batch)\n",
    "            value_preds_batch = _flatten_helper(T, N, value_preds_batch)\n",
    "            return_batch = _flatten_helper(T, N, return_batch)\n",
    "            masks_batch = _flatten_helper(T, N, masks_batch)\n",
    "            old_action_log_probs_batch = _flatten_helper(T, N, \\\n",
    "                    old_action_log_probs_batch)\n",
    "            adv_targ = _flatten_helper(T, N, adv_targ)\n",
    "\n",
    "            yield obs_batch, recurrent_hidden_states_batch, actions_batch, \\\n",
    "                value_preds_batch, return_batch, masks_batch, old_action_log_probs_batch, adv_targ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_STEP = 850  # Cap the number of steps to run for each env\n",
    "NUM_PROCESSES = 8  # Number of processes to use\n",
    "ENV_DIM = 4  # Dimension of one size of the building zone in the env\n",
    "NUM_ACTION = 8  # Number of actions in the action space\n",
    "NUM_EPISODE = 15000\n",
    "NUM_STEP_TOTAL = NUM_STEP * NUM_EPISODE \n",
    "GAMMA = 0.99  # Discount factor\n",
    "VALUE_COEFF = 0.1  # Value loss coefficient\n",
    "ENTROPY_COEFF = 0.5  # Entropy regularisation coefficient\n",
    "LR = 0.00005\n",
    "RMS_EPSILON = 1e-08\n",
    "RMS_ALPHA = 0.99\n",
    "ENTROPY_COEFF_START = 1\n",
    "ENTROPY_COEFF_END = 0.01\n",
    "\n",
    "RECURRENT_HIDDEN_SIZE = 1024  # Size of the hidden state in the actor-critic model\n",
    "USE_LR_DECAY = False\n",
    "\n",
    "replay_buffer = ReplayBuffer((4, ENV_DIM, ENV_DIM, ENV_DIM), 8192, 128, 1, GAMMA)\n",
    "Transition = namedtuple(\"Transition\", (\"state\", \"action\", \"next_state\", \"reward\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init(module, weight_init, bias_init, gain=1):\n",
    "    weight_init(module.weight.data, gain=gain)\n",
    "    bias_init(module.bias.data)\n",
    "    return module\n",
    "\n",
    "# Categorical\n",
    "class FixedCategorical(torch.distributions.Categorical):\n",
    "    def sample(self):\n",
    "        return super().sample().unsqueeze(-1)\n",
    "\n",
    "    def log_probs(self, actions):\n",
    "        return (\n",
    "            super()\n",
    "            .log_prob(actions.squeeze(-1))\n",
    "            .view(actions.size(0), -1)\n",
    "            .sum(-1)\n",
    "            .unsqueeze(-1)\n",
    "        )\n",
    "\n",
    "    def mode(self):\n",
    "        return self.probs.argmax(dim=-1, keepdim=True)\n",
    "\n",
    "class Categorical(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super(Categorical, self).__init__()\n",
    "\n",
    "        \"\"\"init_ = lambda m: init(\n",
    "            m,\n",
    "            nn.init.orthogonal_,\n",
    "            lambda x: nn.init.constant_(x, 0),\n",
    "            gain=0.01)\n",
    "\n",
    "        self.linear = init_(nn.Linear(num_inputs, num_outputs))\"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.linear(x)\n",
    "        x = nn.Softmax(dim=1)(x)\n",
    "        #print(x)\n",
    "        return FixedCategorical(probs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, obs_shape, action_space, base, base_kwargs=None):\n",
    "        super(Policy, self).__init__()\n",
    "        if base_kwargs is None:\n",
    "            base_kwargs = {}\n",
    "\n",
    "        self.base = base(obs_shape[-1], **base_kwargs)\n",
    "        #print(\"action_space\", action_space.__class__.__name__)\n",
    "        #if action_space.__class__.__name__ == \"Discrete\":\n",
    "        num_outputs = NUM_ACTION\n",
    "        self.dist = Categorical(self.base.output_size, num_outputs)\n",
    "        #else:\n",
    "           # raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def is_recurrent(self):\n",
    "        return self.base.is_recurrent\n",
    "\n",
    "    @property\n",
    "    def recurrent_hidden_state_size(self):\n",
    "        \"\"\"Size of rnn_hx.\"\"\"\n",
    "        return self.base.recurrent_hidden_state_size\n",
    "\n",
    "    def forward(self, inputs, rnn_hxs, masks):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def act(self, inputs, rnn_hxs, masks, deterministic=False):\n",
    "        value, actor_features, rnn_hxs = self.base(inputs, rnn_hxs, masks)\n",
    "        dist = self.dist(actor_features)\n",
    "\n",
    "        if deterministic:\n",
    "            action = dist.mode()\n",
    "        else:\n",
    "            action = dist.sample()\n",
    "\n",
    "        action_log_probs = dist.log_probs(action)\n",
    "        #dist_entropy = dist.entropy().mean()\n",
    "\n",
    "        return value, action, action_log_probs, rnn_hxs\n",
    "\n",
    "    def get_value(self, inputs, rnn_hxs, masks):\n",
    "        value, _, _ = self.base(inputs, rnn_hxs, masks)\n",
    "        return value\n",
    "\n",
    "    def evaluate_actions(self, inputs, rnn_hxs, masks, action):\n",
    "        value, actor_features, rnn_hxs = self.base(inputs, rnn_hxs, masks)\n",
    "        dist = self.dist(actor_features)\n",
    "\n",
    "        action_log_probs = dist.log_probs(action)\n",
    "        dist_entropy = dist.entropy().mean()\n",
    "\n",
    "        return value, action_log_probs, dist_entropy, rnn_hxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNBase(nn.Module):\n",
    "    def __init__(self, recurrent, recurrent_input_size, hidden_size):\n",
    "        super(NNBase, self).__init__()\n",
    "\n",
    "        self._hidden_size = hidden_size\n",
    "        self._recurrent = recurrent\n",
    "\n",
    "        if recurrent:\n",
    "            self.gru = nn.GRU(recurrent_input_size, hidden_size)\n",
    "            for name, param in self.gru.named_parameters():\n",
    "                if 'bias' in name:\n",
    "                    nn.init.constant_(param, 0)\n",
    "                elif 'weight' in name:\n",
    "                    nn.init.orthogonal_(param)\n",
    "\n",
    "    @property\n",
    "    def is_recurrent(self):\n",
    "        return self._recurrent\n",
    "\n",
    "    @property\n",
    "    def recurrent_hidden_state_size(self):\n",
    "        if self._recurrent:\n",
    "            return self._hidden_size\n",
    "        return 1\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._hidden_size\n",
    "\n",
    "    def _forward_gru(self, x, hxs, masks):\n",
    "        if x.size(0) == hxs.size(0):\n",
    "            x, hxs = self.gru(x.unsqueeze(0), (hxs * masks).unsqueeze(0))\n",
    "            x = x.squeeze(0)\n",
    "            hxs = hxs.squeeze(0)\n",
    "        else:\n",
    "            # x is a (T, N, -1) tensor that has been flatten to (T * N, -1)\n",
    "            N = hxs.size(0)\n",
    "            T = int(x.size(0) / N)\n",
    "\n",
    "            # unflatten\n",
    "            x = x.view(T, N, x.size(1))\n",
    "\n",
    "            # Same deal with masks\n",
    "            masks = masks.view(T, N)\n",
    "\n",
    "            # Let's figure out which steps in the sequence have a zero for any agent\n",
    "            # We will always assume t=0 has a zero in it as that makes the logic cleaner\n",
    "            has_zeros = ((masks[1:] == 0.0) \\\n",
    "                            .any(dim=-1)\n",
    "                            .nonzero()\n",
    "                            .squeeze()\n",
    "                            .cpu())\n",
    "\n",
    "            # +1 to correct the masks[1:]\n",
    "            if has_zeros.dim() == 0:\n",
    "                # Deal with scalar\n",
    "                has_zeros = [has_zeros.item() + 1]\n",
    "            else:\n",
    "                has_zeros = (has_zeros + 1).numpy().tolist()\n",
    "\n",
    "            # add t=0 and t=T to the list\n",
    "            has_zeros = [0] + has_zeros + [T]\n",
    "\n",
    "            hxs = hxs.unsqueeze(0)\n",
    "            outputs = []\n",
    "            for i in range(len(has_zeros) - 1):\n",
    "                # We can now process steps that don't have any zeros in masks together!\n",
    "                # This is much faster\n",
    "                start_idx = has_zeros[i]\n",
    "                end_idx = has_zeros[i + 1]\n",
    "\n",
    "                rnn_scores, hxs = self.gru(\n",
    "                    x[start_idx:end_idx],\n",
    "                    hxs * masks[start_idx].view(1, -1, 1))\n",
    "\n",
    "                outputs.append(rnn_scores)\n",
    "\n",
    "            # assert len(outputs) == T\n",
    "            # x is a (T, N, -1) tensor\n",
    "            x = torch.cat(outputs, dim=0)\n",
    "            # flatten\n",
    "            x = x.view(T * N, -1)\n",
    "            hxs = hxs.squeeze(0)\n",
    "\n",
    "        return x, hxs\n",
    "\n",
    "class CNNBase(NNBase):\n",
    "    def __init__(self, num_inputs, recurrent=True, hidden_size=1024):\n",
    "        super(CNNBase, self).__init__(recurrent, hidden_size, hidden_size)\n",
    "\n",
    "        init_ = lambda m: init(m, nn.init.orthogonal_, lambda x: nn.init.\n",
    "                               constant_(x, 0), nn.init.calculate_gain('relu'))\n",
    "\n",
    "        \n",
    "        self.conv1 = init_(nn.Conv3d(3, 29, 3, 1, 1))\n",
    "        self.conv2 = init_(nn.Conv3d(32, 67, 3, 1, 1))\n",
    "        self.fc1 = init_(nn.Linear((67 + 3) * ENV_DIM * ENV_DIM * ENV_DIM, 1024))\n",
    "        \n",
    "        init_ = lambda m: init(m, nn.init.orthogonal_, lambda x: nn.init.\n",
    "                               constant_(x, 0))\n",
    "        self.critic_linear = init_(nn.Linear(hidden_size, 1))\n",
    "        self.actor_linear = init_(nn.Linear(hidden_size, NUM_ACTION))\n",
    "        self.train()\n",
    "\n",
    "    def forward(self, inputs, rnn_hxs, masks):\n",
    "        original_input = inputs\n",
    "        \n",
    "        x = self.conv1(original_input)\n",
    "        x = torch.cat((x, original_input), dim=1)\n",
    "        x = nn.ReLU()(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = torch.cat((x, original_input), dim=1)\n",
    "        x = nn.ReLU()(x)\n",
    "        #print(x.shape)\n",
    "        x = nn.Flatten()(x)\n",
    "        x = self.fc1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        \n",
    "        \n",
    "        if self.is_recurrent:\n",
    "            x, rnn_hxs = self._forward_gru(x, rnn_hxs, masks)\n",
    "\n",
    "        return self.critic_linear(x), self.actor_linear(x), rnn_hxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tutha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "c:\\Users\\tutha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.get_obs to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.get_obs` for environment variables or `env.get_wrapper_attr('get_obs')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\tutha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:228: UserWarning: \u001b[33mWARN: Expects `terminated` signal to be a boolean, actual type: <class 'numpy.ndarray'>\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\tutha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:232: UserWarning: \u001b[33mWARN: Expects `truncated` signal to be a boolean, actual type: <class 'numpy.ndarray'>\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\tutha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "c:\\Users\\tutha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:246: UserWarning: \u001b[33mWARN: The reward returned by `step()` must be a float, int, np.integer or np.floating, actual type: <class 'numpy.ndarray'>\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "PPO.update() missing 1 required positional argument: 'entropy_coefficent'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 172\u001b[0m\n\u001b[0;32m    168\u001b[0m             env\u001b[38;5;241m.\u001b[39mrender()\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 172\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[22], line 144\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;66;03m# Linearly decrease the entropy coefficient\u001b[39;00m\n\u001b[0;32m    142\u001b[0m entropy_coefficent \u001b[38;5;241m=\u001b[39m ENTROPY_COEFF_START \u001b[38;5;241m+\u001b[39m (ENTROPY_COEFF_END \u001b[38;5;241m-\u001b[39m ENTROPY_COEFF_START) \u001b[38;5;241m*\u001b[39m j \u001b[38;5;241m/\u001b[39m num_updates\n\u001b[1;32m--> 144\u001b[0m value_loss, action_loss, dist_entropy \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrollouts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    146\u001b[0m rollouts\u001b[38;5;241m.\u001b[39mafter_update()\n\u001b[0;32m    148\u001b[0m \u001b[38;5;66;03m# save for every interval-th episode or for the last epoch\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: PPO.update() missing 1 required positional argument: 'entropy_coefficent'"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import GridWorld_env\n",
    "def main():\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    n_actions = 8\n",
    "    env = gym.make(\"GridWorld_env/GridWorld\", dimension_size=4, path=\"targets\", batch_size=NUM_PROCESSES)\n",
    "    env.reset()\n",
    "\n",
    "\n",
    "    actor_critic = Policy(\n",
    "        obs_shape=(3, ENV_DIM, ENV_DIM, ENV_DIM),\n",
    "        action_space=n_actions,\n",
    "        base=CNNBase,\n",
    "        )\n",
    "    actor_critic.to(device)\n",
    "    \n",
    "    \n",
    "    \"\"\"agent = A2C(\n",
    "        actor_critic=actor_critic,\n",
    "        value_loss_coef=VALUE_COEFF,\n",
    "        entropy_coef=ENTROPY_COEFF,\n",
    "        lr=LR,\n",
    "        eps=RMS_EPSILON,\n",
    "        alpha=RMS_ALPHA,\n",
    "        max_grad_norm=0.5\n",
    "    )\"\"\"\n",
    "    agent = PPO(\n",
    "            actor_critic,\n",
    "            clip_param=0.2,\n",
    "            ppo_epoch=10,\n",
    "            num_mini_batch=NUM_PROCESSES,\n",
    "            value_loss_coef=VALUE_COEFF,\n",
    "            entropy_coef=ENTROPY_COEFF,\n",
    "            lr=LR,\n",
    "            eps=1e-8,\n",
    "            max_grad_norm=0.5)\n",
    "    \"\"\"\n",
    "    if args.gail:\n",
    "        assert len(envs.observation_space.shape) == 1\n",
    "        discr = gail.Discriminator(\n",
    "            envs.observation_space.shape[0] + envs.action_space.shape[0], 100,\n",
    "            device)\n",
    "        file_name = os.path.join(\n",
    "            args.gail_experts_dir, \"trajs_{}.pt\".format(\n",
    "                args.env_name.split('-')[0].lower()))\n",
    "        \n",
    "        expert_dataset = gail.ExpertDataset(\n",
    "            file_name, num_trajectories=4, subsample_frequency=20)\n",
    "        drop_last = len(expert_dataset) > args.gail_batch_size\n",
    "        gail_train_loader = torch.utils.data.DataLoader(\n",
    "            dataset=expert_dataset,\n",
    "            batch_size=args.gail_batch_size,\n",
    "            shuffle=True,\n",
    "            drop_last=drop_last)\"\"\"\n",
    "\n",
    "    rollouts = RolloutStorage(num_steps=NUM_STEP, num_processes=NUM_PROCESSES,\n",
    "                              obs_shape=(3, ENV_DIM, ENV_DIM, ENV_DIM), action_space=NUM_ACTION,\n",
    "                              recurrent_hidden_state_size=RECURRENT_HIDDEN_SIZE)\n",
    "\n",
    "    obs = env.get_obs()\n",
    "    obs = torch.from_numpy(obs).float()\n",
    "    rollouts.obs[0].copy_(obs)\n",
    "    rollouts.to(device)\n",
    "\n",
    "    episode_rewards = deque(maxlen=50)\n",
    "\n",
    "    start = time.time()\n",
    "    num_updates = int(\n",
    "        NUM_STEP_TOTAL) // NUM_STEP // NUM_PROCESSES\n",
    "    print(num_updates)\n",
    "    for j in range(num_updates):\n",
    "        env.reset()\n",
    "        \"\"\"if USE_LR_DECAY:\n",
    "            # decrease learning rate linearly\n",
    "            utils.update_linear_schedule(\n",
    "                agent.optimizer, j, num_updates,\n",
    "                agent.optimizer.lr if args.algo == \"acktr\" else args.lr)\"\"\"\n",
    "\n",
    "        for step in range(NUM_STEP):\n",
    "            # Sample actions\n",
    "            with torch.no_grad():\n",
    "                value, action, action_log_prob, recurrent_hidden_states = actor_critic.act(\n",
    "                    rollouts.obs[step], rollouts.recurrent_hidden_states[step],\n",
    "                    rollouts.masks[step])\n",
    "            #print(rollouts.obs[step].shape, rollouts.recurrent_hidden_states[step].shape, rollouts.masks[step].shape)\n",
    "            # Obser reward and next obs\n",
    "            obs, reward, done, truncated, infos = env.step(action)\n",
    "            obs = torch.from_numpy(obs).float()\n",
    "            reward = torch.from_numpy(reward).unsqueeze(-1).float()\n",
    "            #done = [done]\n",
    "            episode_rewards.append(reward)\n",
    "\n",
    "            # If done then clean the history of observations.\n",
    "            masks = torch.FloatTensor(\n",
    "                [[0.0] if done_ else [1.0] for done_ in done])\n",
    "            \"\"\"bad_masks = torch.FloatTensor(\n",
    "                [[0.0] if 'bad_transition' in info.keys() else [1.0]\n",
    "                 for info in infos])\"\"\"\n",
    "                 \n",
    "            bad_masks = torch.FloatTensor(\n",
    "                [[0.0] if done_ else [1.0] for done_ in done])\n",
    "            \n",
    "            \n",
    "            rollouts.insert(obs, recurrent_hidden_states, action,\n",
    "                            action_log_prob, value, reward, masks, bad_masks)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_value = actor_critic.get_value(\n",
    "                rollouts.obs[-1], rollouts.recurrent_hidden_states[-1],\n",
    "                rollouts.masks[-1]).detach()\n",
    "\n",
    "        \"\"\"if args.gail:\n",
    "            if j >= 10:\n",
    "                env.venv.eval()\n",
    "\n",
    "            gail_epoch = args.gail_epoch\n",
    "            if j < 10:\n",
    "                gail_epoch = 100  # Warm up\n",
    "            for _ in range(gail_epoch):\n",
    "                discr.update(gail_train_loader, rollouts,\n",
    "                             utils.get_vec_normalize(env)._obfilt)\n",
    "\n",
    "            for step in range(NUM_STEP):\n",
    "                rollouts.rewards[step] = discr.predict_reward(\n",
    "                    rollouts.obs[step], rollouts.actions[step], args.gamma,\n",
    "                    rollouts.masks[step])\"\"\"\n",
    "\n",
    "        rollouts.compute_returns(next_value, use_gae=True, gamma=GAMMA,\n",
    "                                 gae_lambda=0.95, use_proper_time_limits=False)\n",
    "        \n",
    "        # Linearly decrease the entropy coefficient\n",
    "        entropy_coefficent = ENTROPY_COEFF_START + (ENTROPY_COEFF_END - ENTROPY_COEFF_START) * j / num_updates\n",
    "        \n",
    "        value_loss, action_loss, dist_entropy = agent.update(rollouts, j, entropy_coefficent)\n",
    "\n",
    "        rollouts.after_update()\n",
    "\n",
    "        # save for every interval-th episode or for the last epoch\n",
    "        if (j % 100 == 0\n",
    "                or j == num_updates - 1):\n",
    "\n",
    "            torch.save(#[\n",
    "                actor_critic,\n",
    "                #getattr(utils.get_vec_normalize(env), 'obs_rms', None)],\n",
    "            \"A2C_checkpoint_\" + str(j) + \".pt\")\n",
    "        \n",
    "        if j % 10 == 0 and len(episode_rewards) > 1:\n",
    "            total_num_steps = (j + 1) * NUM_PROCESSES * NUM_STEP\n",
    "            end = time.time()\n",
    "            print(\n",
    "                \"Updates {}, num timesteps {}, FPS {} \\n Last {} training episodes: mean/median reward {:.1f}/{:.1f}, min/max reward {:.1f}/{:.1f}\\n\"\n",
    "                .format(j, total_num_steps,\n",
    "                        int(total_num_steps / (end - start)),\n",
    "                        len(episode_rewards), np.mean(episode_rewards),\n",
    "                        np.median(episode_rewards), np.min(episode_rewards),\n",
    "                        np.max(episode_rewards)))\n",
    "            print(\"value_loss\", value_loss, \"action_loss\", action_loss, \"dist_entropy\", dist_entropy)\n",
    "            env.render()\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"GridWorld_env/GridWorld\", dimension_size=4, path=\"targets\")\n",
    "from IPython import display\n",
    "env.reset()\n",
    "device = 'cuda'\n",
    "actor_critic = torch.load(\"A2C_checkpoint_999.pt\")\n",
    "rollouts = RolloutStorage(num_steps=NUM_STEP, num_processes=NUM_PROCESSES,\n",
    "                            obs_shape=(3, ENV_DIM, ENV_DIM, ENV_DIM), action_space=NUM_ACTION,\n",
    "                            recurrent_hidden_state_size=RECURRENT_HIDDEN_SIZE)\n",
    "\n",
    "obs = env.get_obs()\n",
    "obs = torch.from_numpy(obs).float()\n",
    "rollouts.obs[0].copy_(obs)\n",
    "rollouts.to(device)\n",
    "\n",
    "for step in range(NUM_STEP):\n",
    "        # Sample actions\n",
    "        with torch.no_grad():\n",
    "            value, action, action_log_prob, recurrent_hidden_states = actor_critic.act(\n",
    "                rollouts.obs[step], rollouts.recurrent_hidden_states[step],\n",
    "                rollouts.masks[step])\n",
    "\n",
    "        # Obser reward and next obs\n",
    "        obs, reward, done, truncated, infos = env.step(action)\n",
    "        env.render()\n",
    "        display.clear_output(wait=True)\n",
    "        obs = torch.from_numpy(obs).float()\n",
    "        \n",
    "        done = [done]\n",
    "\n",
    "        # If done then clean the history of observations.\n",
    "        masks = torch.FloatTensor(\n",
    "            [[0.0] if done_ else [1.0] for done_ in done])\n",
    "        \"\"\"bad_masks = torch.FloatTensor(\n",
    "            [[0.0] if 'bad_transition' in info.keys() else [1.0]\n",
    "                for info in infos])\"\"\"\n",
    "                \n",
    "        bad_masks = torch.FloatTensor(\n",
    "            [[0.0] if done_ else [1.0] for done_ in done])\n",
    "        \n",
    "        rollouts.insert(obs, recurrent_hidden_states, action,\n",
    "                        action_log_prob, value, reward, masks, bad_masks)\n",
    "\n",
    "with torch.no_grad():\n",
    "    next_value = actor_critic.get_value(\n",
    "        rollouts.obs[-1], rollouts.recurrent_hidden_states[-1],\n",
    "        rollouts.masks[-1]).detach()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env.reset()\n",
    "for i in count():\n",
    "    \n",
    "    action, _, _, _, _ = net(torch.from_numpy(env.get_obs()).unsqueeze(0).float().cuda())\n",
    "    env.step(action.cpu().numpy())\n",
    "    env.render()\n",
    "    display.clear_output(wait=True)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
