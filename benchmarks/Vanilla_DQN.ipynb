{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jEzq6HriN3G",
        "outputId": "cd1022f6-7a34-411e-d379-cf004453becf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/Deep-Reinforcement-Learning-agent-for-Construction-Project-Execution/GridWorld-env\n",
            "Obtaining file:///content/drive/MyDrive/Deep-Reinforcement-Learning-agent-for-Construction-Project-Execution/GridWorld-env\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gymnasium==0.29.1 in /usr/local/lib/python3.10/dist-packages (from GridWorld-env==0.0.1) (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.29.1->GridWorld-env==0.0.1) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.29.1->GridWorld-env==0.0.1) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.29.1->GridWorld-env==0.0.1) (4.10.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.29.1->GridWorld-env==0.0.1) (0.0.4)\n",
            "Installing collected packages: GridWorld-env\n",
            "  Attempting uninstall: GridWorld-env\n",
            "    Found existing installation: GridWorld-env 0.0.1\n",
            "    Uninstalling GridWorld-env-0.0.1:\n",
            "      Successfully uninstalled GridWorld-env-0.0.1\n",
            "  Running setup.py develop for GridWorld-env\n",
            "Successfully installed GridWorld-env-0.0.1\n",
            "/content/drive/MyDrive/Deep-Reinforcement-Learning-agent-for-Construction-Project-Execution/benchmarks\n",
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.10/dist-packages (1.8.0)\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd \"/content/drive/MyDrive/Deep-Reinforcement-Learning-agent-for-Construction-Project-Execution/GridWorld-env\"\n",
        "!pip install -e .\n",
        "import GridWorld_env\n",
        "\n",
        "%cd \"/content/drive/MyDrive/Deep-Reinforcement-Learning-agent-for-Construction-Project-Execution/benchmarks\"\n",
        "!pip install torchinfo\n",
        "\n",
        "import matplotlib\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import GridWorld_env\n",
        "from replay_buffer import ReplayBuffer\n",
        "import gymnasium as gym\n",
        "import random\n",
        "import math\n",
        "from itertools import count\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "is_ipython = \"inline\" in matplotlib.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "\n",
        "from collections import deque, namedtuple\n",
        "\n",
        "Transition = namedtuple(\"Transition\", (\"state\", \"action\", \"next_state\", \"reward\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3MqJYjfiN3H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecc73af3-2222-4357-96dc-3c416f875366"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==========================================================================================\n",
            "Layer (type:depth-idx)                   Output Shape              Param #\n",
            "==========================================================================================\n",
            "Vanilla_DQN                              [1, 8]                    --\n",
            "├─Linear: 1-1                            [1, 1024]                 197,632\n",
            "├─Linear: 1-2                            [1, 1024]                 1,049,600\n",
            "├─Linear: 1-3                            [1, 1024]                 1,049,600\n",
            "├─Linear: 1-4                            [1, 1024]                 1,049,600\n",
            "├─Linear: 1-5                            [1, 1024]                 1,049,600\n",
            "├─Linear: 1-6                            [1, 8]                    8,200\n",
            "==========================================================================================\n",
            "Total params: 4,404,232\n",
            "Trainable params: 4,404,232\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (M): 4.40\n",
            "==========================================================================================\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.04\n",
            "Params size (MB): 17.62\n",
            "Estimated Total Size (MB): 17.66\n",
            "==========================================================================================\n"
          ]
        }
      ],
      "source": [
        "class Vanilla_DQN(nn.Module):\n",
        "    def __init__(self, input_dim, action_dim):\n",
        "        super(Vanilla_DQN, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(192, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 1024)\n",
        "        self.fc3 = nn.Linear(1024, 1024)\n",
        "        self.fc4 = nn.Linear(1024, 1024)\n",
        "        self.fc5 = nn.Linear(1024, 1024)\n",
        "\n",
        "        self.actions = nn.Linear(1024, action_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = nn.Flatten()(x)\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = x + self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = x + self.fc3(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = x + self.fc4(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = x + self.fc5(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        actions = self.actions(x)\n",
        "\n",
        "        return actions\n",
        "\n",
        "test = Vanilla_DQN(4, 8)\n",
        "import torchinfo\n",
        "print(torchinfo.summary(test, (1, 3, 4, 4, 4)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bem9tEYPiN3H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2da97b7c-ca87-4b87-d9b8-e4fdafaeaf5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gymnasium/utils/passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
            "  logger.warn(f\"{pre} is not within the observation space.\")\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 32\n",
        "GAMMA = 0.9\n",
        "EPS_START = 0.99\n",
        "EPS_END = 0.01\n",
        "EPS_DECAY = 150000\n",
        "TAU = 0.0003\n",
        "STEPSIZE = 0.0000625\n",
        "BETA_START = 0.4\n",
        "BETA_END = 1\n",
        "BETA_LINEAR_CAP = 500 * 850\n",
        "N_STEP = 1\n",
        "\n",
        "n_actions = 8\n",
        "env = gym.make(\"GridWorld_env/GridWorld\", dimension_size=4, path=\"targets\")\n",
        "env.reset()\n",
        "\n",
        "policy_net = Vanilla_DQN(4, 8)\n",
        "target_net = Vanilla_DQN(4, 8)\n",
        "\n",
        "\n",
        "policy_net.cuda()\n",
        "target_net.cuda()\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "optimiser = optim.Adam(policy_net.parameters(), lr=STEPSIZE, eps=1.5e-4)\n",
        "memory = ReplayBuffer(obs_dim=(3,4,4,4), size=8192, n_step=N_STEP, gamma = GAMMA)\n",
        "\n",
        "steps_done = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-pKzQOUiN3I"
      },
      "outputs": [],
      "source": [
        "def select_action(state, greedy = False):\n",
        "    global steps_done\n",
        "\n",
        "    sample = random.random()\n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
        "    steps_done += 1\n",
        "\n",
        "    if greedy:\n",
        "        return policy_net(state).max(1).indices.view(1,1)\n",
        "\n",
        "    if sample > eps_threshold:\n",
        "        with torch.no_grad():\n",
        "            return policy_net(state).max(1).indices.view(1,1)\n",
        "    else:\n",
        "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkPWuao2iN3I"
      },
      "outputs": [],
      "source": [
        "episode_durations = []\n",
        "\n",
        "def plot_durations(show_result = False):\n",
        "    plt.figure(1)\n",
        "\n",
        "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
        "    if show_result:\n",
        "        plt.title(\"Result\")\n",
        "    else:\n",
        "        plt.clf()\n",
        "        plt.title(\"Training\")\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Duration\")\n",
        "\n",
        "    plt.plot(durations_t.numpy())\n",
        "\n",
        "    if len(durations_t) >= 100:\n",
        "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
        "        means = torch.cat((torch.zeros(99), means))\n",
        "        plt.plot(means.numpy())\n",
        "\n",
        "        plt.pause(0.001)\n",
        "        if is_ipython:\n",
        "            if not show_result:\n",
        "                display.display(plt.gcf())\n",
        "                display.clear_output(wait=True)\n",
        "            else:\n",
        "                display.display(plt.gcf())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5AM0pjRiN3I"
      },
      "outputs": [],
      "source": [
        "def optimise_model(beta):\n",
        "    if len(memory) < BATCH_SIZE * 64:\n",
        "        return 0, 0\n",
        "    transitions = memory.sample_batch()\n",
        "    batch = Transition(\n",
        "        torch.tensor(transitions[\"obs\"], device=device),\n",
        "        torch.tensor(transitions[\"acts\"], device=device, dtype=torch.int64),\n",
        "        torch.tensor(transitions[\"next_obs\"], device=device),\n",
        "        torch.tensor(transitions[\"rews\"], device=device)\n",
        "    )\n",
        "\n",
        "    indices = transitions[\"indices\"]\n",
        "\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
        "    non_final_next_states = torch.cat([s.unsqueeze(0) for s in batch.next_state if s is not None])\n",
        "\n",
        "    state_batch = batch.state\n",
        "    action_batch = batch.action\n",
        "    reward_batch = batch.reward\n",
        "\n",
        "    tmp = policy_net(state_batch)\n",
        "    state_action_values = tmp.gather(1, action_batch.unsqueeze(1))\n",
        "    next_state_values = torch.zeros(BATCH_SIZE, device = device)\n",
        "\n",
        "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
        "    with torch.no_grad():\n",
        "        expected_state_action_values = (GAMMA ** N_STEP) * next_state_values + reward_batch\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "\n",
        "    loss = criterion(state_action_values , expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "    optimiser.zero_grad()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
        "    optimiser.step()\n",
        "\n",
        "    new_priorities = torch.abs(state_action_values - expected_state_action_values.unsqueeze(1)).detach().cpu().numpy() + 1e-7 #loss.detach().cpu().numpy() + 1e-6\n",
        "    return loss.item(), reward_batch.float()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMxqS6KmiN3I"
      },
      "outputs": [],
      "source": [
        "reward_plot = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArKQM3qBiN3I"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    num_episodes = 10\n",
        "else:\n",
        "    num_episodes = 1\n",
        "\n",
        "for i_episode in range(num_episodes):\n",
        "    state, info = env.reset()\n",
        "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "    cummulative_reward = 0\n",
        "    for t in count():\n",
        "        action = select_action(state)\n",
        "\n",
        "\n",
        "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
        "\n",
        "        cummulative_reward += reward\n",
        "        reward = torch.tensor([reward], device=device)\n",
        "\n",
        "        done = terminated or truncated\n",
        "        if terminated:\n",
        "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "        else:\n",
        "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "\n",
        "        memory.store(state.cpu().numpy(), action, reward, next_state.cpu().numpy(), done)\n",
        "        state = next_state\n",
        "\n",
        "        beta = BETA_START + (BETA_END - BETA_START) * (steps_done) / BETA_LINEAR_CAP if steps_done < BETA_LINEAR_CAP else BETA_END\n",
        "        l, r = optimise_model(beta)\n",
        "\n",
        "        # Soft update of the target network's weights\n",
        "        # θ′ ← τ θ + (1 −τ )θ′\n",
        "        target_net_state_dict = target_net.state_dict()\n",
        "        policy_net_state_dict = policy_net.state_dict()\n",
        "        for key in policy_net_state_dict:\n",
        "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
        "        target_net.load_state_dict(target_net_state_dict)\n",
        "\n",
        "        if done:\n",
        "            episode_durations.append(t + 1)\n",
        "            plot_durations()\n",
        "            break\n",
        "\n",
        "    if i_episode % 1 == 0 and i_episode > 1:\n",
        "        print(\"Episode: {0} Loss {1} Mean Sample Reward {2}:\" .format(i_episode, l, r.mean().item()))\n",
        "        #print(env.unwrapped.get_seqsuence())\n",
        "        env.unwrapped.render()\n",
        "\n",
        "    reward_plot.append(cummulative_reward)\n",
        "\n",
        "print('Complete')\n",
        "plot_durations(show_result=True)\n",
        "plt.ioff()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6o_wcXuiN3I"
      },
      "outputs": [],
      "source": [
        "torch.save(policy_net, \"vanilla_dqn_policy.pt\")\n",
        "torch.save(target_net, \"vanilla_dqn_target.pt\")\n",
        "np.save(\"reward_plot.npy\", reward_plot)\n",
        "np.save(\"episode_durations.npy\", episode_durations)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}