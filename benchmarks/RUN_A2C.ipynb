{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
    "\n",
    "\n",
    "NUM_STEP = 850  # Cap the number of steps to run for each env\n",
    "NUM_PROCESSES = 2 # Number of processes to use\n",
    "ENV_DIM = 4  # Dimension of one size of the building zone in the env\n",
    "NUM_ACTION = 8  # Number of actions in the action space\n",
    "NUM_EPISODE = 2500\n",
    "NUM_STEP_TOTAL = NUM_STEP * NUM_EPISODE * NUM_PROCESSES\n",
    "GAMMA = 0.99  # Discount factor\n",
    "VALUE_COEFF = 0.1  # Value loss coefficient\n",
    "ENTROPY_COEFF = 0.5  # Entropy regularisation coefficient\n",
    "LR = 0.00005\n",
    "RMS_EPSILON = 1e-08\n",
    "RMS_ALPHA = 0.99\n",
    "ENTROPY_COEFF_START = 1\n",
    "ENTROPY_COEFF_END = 0.01\n",
    "\n",
    "RECURRENT_HIDDEN_SIZE = 1024  # Size of the hidden state in the actor-critic model\n",
    "USE_LR_DECAY = False\n",
    "\n",
    "\n",
    "\n",
    "# Categorical\n",
    "class FixedCategorical(torch.distributions.Categorical):\n",
    "    def sample(self):\n",
    "        return super().sample().unsqueeze(-1)\n",
    "\n",
    "    def log_probs(self, actions):\n",
    "        return (\n",
    "            super()\n",
    "            .log_prob(actions.squeeze(-1))\n",
    "            .view(actions.size(0), -1)\n",
    "            .sum(-1)\n",
    "            .unsqueeze(-1)\n",
    "        )\n",
    "\n",
    "    def mode(self):\n",
    "        return self.probs.argmax(dim=-1, keepdim=True)\n",
    "\n",
    "class Categorical(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super(Categorical, self).__init__()\n",
    "\n",
    "        \"\"\"init_ = lambda m: init(\n",
    "            m,\n",
    "            nn.init.orthogonal_,\n",
    "            lambda x: nn.init.constant_(x, 0),\n",
    "            gain=0.01)\n",
    "\n",
    "        self.linear = init_(nn.Linear(num_inputs, num_outputs))\"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.linear(x)\n",
    "        x = nn.Softmax(dim=1)(x)\n",
    "        print(x)\n",
    "        return FixedCategorical(probs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def init(module, weight_init, bias_init, gain=1):\n",
    "    weight_init(module.weight.data, gain=gain)\n",
    "    bias_init(module.bias.data)\n",
    "    return module\n",
    "\n",
    "\n",
    "class NNBase(nn.Module):\n",
    "    def __init__(self, recurrent, recurrent_input_size, hidden_size):\n",
    "        super(NNBase, self).__init__()\n",
    "\n",
    "        self._hidden_size = hidden_size\n",
    "        self._recurrent = recurrent\n",
    "\n",
    "        if recurrent:\n",
    "            self.gru = nn.GRU(recurrent_input_size, hidden_size)\n",
    "            for name, param in self.gru.named_parameters():\n",
    "                if 'bias' in name:\n",
    "                    nn.init.constant_(param, 0)\n",
    "                elif 'weight' in name:\n",
    "                    nn.init.orthogonal_(param)\n",
    "\n",
    "    @property\n",
    "    def is_recurrent(self):\n",
    "        return self._recurrent\n",
    "\n",
    "    @property\n",
    "    def recurrent_hidden_state_size(self):\n",
    "        if self._recurrent:\n",
    "            return self._hidden_size\n",
    "        return 1\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._hidden_size\n",
    "\n",
    "    def _forward_gru(self, x, hxs, masks):\n",
    "        if x.size(0) == hxs.size(0):\n",
    "            x, hxs = self.gru(x.unsqueeze(0), (hxs * masks).unsqueeze(0))\n",
    "            x = x.squeeze(0)\n",
    "            hxs = hxs.squeeze(0)\n",
    "        else:\n",
    "            # x is a (T, N, -1) tensor that has been flatten to (T * N, -1)\n",
    "            N = hxs.size(0)\n",
    "            T = int(x.size(0) / N)\n",
    "\n",
    "            # unflatten\n",
    "            x = x.view(T, N, x.size(1))\n",
    "\n",
    "            # Same deal with masks\n",
    "            masks = masks.view(T, N)\n",
    "\n",
    "            # Let's figure out which steps in the sequence have a zero for any agent\n",
    "            # We will always assume t=0 has a zero in it as that makes the logic cleaner\n",
    "            has_zeros = ((masks[1:] == 0.0) \\\n",
    "                            .any(dim=-1)\n",
    "                            .nonzero()\n",
    "                            .squeeze()\n",
    "                            .cpu())\n",
    "\n",
    "            # +1 to correct the masks[1:]\n",
    "            if has_zeros.dim() == 0:\n",
    "                # Deal with scalar\n",
    "                has_zeros = [has_zeros.item() + 1]\n",
    "            else:\n",
    "                has_zeros = (has_zeros + 1).numpy().tolist()\n",
    "\n",
    "            # add t=0 and t=T to the list\n",
    "            has_zeros = [0] + has_zeros + [T]\n",
    "\n",
    "            hxs = hxs.unsqueeze(0)\n",
    "            outputs = []\n",
    "            for i in range(len(has_zeros) - 1):\n",
    "                # We can now process steps that don't have any zeros in masks together!\n",
    "                # This is much faster\n",
    "                start_idx = has_zeros[i]\n",
    "                end_idx = has_zeros[i + 1]\n",
    "\n",
    "                rnn_scores, hxs = self.gru(\n",
    "                    x[start_idx:end_idx],\n",
    "                    hxs * masks[start_idx].view(1, -1, 1))\n",
    "\n",
    "                outputs.append(rnn_scores)\n",
    "\n",
    "            # assert len(outputs) == T\n",
    "            # x is a (T, N, -1) tensor\n",
    "            x = torch.cat(outputs, dim=0)\n",
    "            # flatten\n",
    "            x = x.view(T * N, -1)\n",
    "            hxs = hxs.squeeze(0)\n",
    "\n",
    "        return x, hxs\n",
    "\n",
    "class CNNBase(NNBase):\n",
    "    def __init__(self, num_inputs, recurrent=True, hidden_size=1024):\n",
    "        super(CNNBase, self).__init__(recurrent, hidden_size, hidden_size)\n",
    "\n",
    "        init_ = lambda m: init(m, nn.init.orthogonal_, lambda x: nn.init.\n",
    "                               constant_(x, 0), nn.init.calculate_gain('relu'))\n",
    "\n",
    "        \n",
    "        self.conv1 = init_(nn.Conv3d(3, 29, 3, 1, 1))\n",
    "        self.conv2 = init_(nn.Conv3d(32, 67, 3, 1, 1))\n",
    "        self.fc1 = init_(nn.Linear((67 + 3) * ENV_DIM * ENV_DIM * ENV_DIM, 1024))\n",
    "        \n",
    "        init_ = lambda m: init(m, nn.init.orthogonal_, lambda x: nn.init.\n",
    "                               constant_(x, 0))\n",
    "        self.critic_linear = init_(nn.Linear(hidden_size, 1))\n",
    "        self.actor_linear = init_(nn.Linear(hidden_size, NUM_ACTION))\n",
    "        self.train()\n",
    "\n",
    "    def forward(self, inputs, rnn_hxs, masks):\n",
    "        original_input = inputs\n",
    "        \n",
    "        x = self.conv1(original_input)\n",
    "        x = torch.cat((x, original_input), dim=1)\n",
    "        x = nn.ReLU()(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = torch.cat((x, original_input), dim=1)\n",
    "        x = nn.ReLU()(x)\n",
    "        #print(x.shape)\n",
    "        x = nn.Flatten()(x)\n",
    "        x = self.fc1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        \n",
    "        \n",
    "        if self.is_recurrent:\n",
    "            x, rnn_hxs = self._forward_gru(x, rnn_hxs, masks)\n",
    "\n",
    "        return self.critic_linear(x), self.actor_linear(x), rnn_hxs\n",
    "\n",
    "class PPO():\n",
    "    def __init__(self,\n",
    "                 actor_critic,\n",
    "                 clip_param,\n",
    "                 ppo_epoch,\n",
    "                 num_mini_batch,\n",
    "                 value_loss_coef,\n",
    "                 entropy_coef,\n",
    "                 lr=None,\n",
    "                 eps=None,\n",
    "                 max_grad_norm=None,\n",
    "                 use_clipped_value_loss=True):\n",
    "\n",
    "        self.actor_critic = actor_critic\n",
    "\n",
    "        self.clip_param = clip_param\n",
    "        self.ppo_epoch = ppo_epoch\n",
    "        self.num_mini_batch = num_mini_batch\n",
    "\n",
    "        self.value_loss_coef = value_loss_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.use_clipped_value_loss = use_clipped_value_loss\n",
    "\n",
    "        self.optimizer = optim.Adam(actor_critic.parameters(), lr=lr, eps=eps)\n",
    "        #self.writer = SummaryWriter()\n",
    "        \n",
    "\n",
    "    def update(self, rollouts, j, entropy_coefficent):\n",
    "        advantages = rollouts.returns[:-1] - rollouts.value_preds[:-1]\n",
    "        advantages = (advantages - advantages.mean()) / (\n",
    "            advantages.std() + 1e-5)\n",
    "\n",
    "        value_loss_epoch = 0\n",
    "        action_loss_epoch = 0\n",
    "        dist_entropy_epoch = 0\n",
    "\n",
    "        for e in range(self.ppo_epoch):\n",
    "            if self.actor_critic.is_recurrent:\n",
    "                data_generator = rollouts.recurrent_generator(\n",
    "                    advantages, self.num_mini_batch)\n",
    "            else:\n",
    "                data_generator = rollouts.feed_forward_generator(\n",
    "                    advantages, self.num_mini_batch)\n",
    "\n",
    "            for sample in data_generator:\n",
    "                obs_batch, recurrent_hidden_states_batch, actions_batch, \\\n",
    "                   value_preds_batch, return_batch, masks_batch, old_action_log_probs_batch, \\\n",
    "                        adv_targ = sample\n",
    "\n",
    "                # Reshape to do in a single forward pass for all steps\n",
    "                values, action_log_probs, dist_entropy, _ = self.actor_critic.evaluate_actions(\n",
    "                    obs_batch, recurrent_hidden_states_batch, masks_batch,\n",
    "                    actions_batch)\n",
    "\n",
    "                ratio = torch.exp(action_log_probs -\n",
    "                                  old_action_log_probs_batch)\n",
    "                surr1 = ratio * adv_targ\n",
    "                surr2 = torch.clamp(ratio, 1.0 - self.clip_param,\n",
    "                                    1.0 + self.clip_param) * adv_targ\n",
    "                action_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "                if self.use_clipped_value_loss:\n",
    "                    value_pred_clipped = value_preds_batch + \\\n",
    "                        (values - value_preds_batch).clamp(-self.clip_param, self.clip_param)\n",
    "                    value_losses = (values - return_batch).pow(2)\n",
    "                    value_losses_clipped = (\n",
    "                        value_pred_clipped - return_batch).pow(2)\n",
    "                    value_loss = 0.5 * torch.max(value_losses,\n",
    "                                                 value_losses_clipped).mean()\n",
    "                else:\n",
    "                    value_loss = 0.5 * (return_batch - values).pow(2).mean()\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                (value_loss * self.value_loss_coef + action_loss -\n",
    "                 dist_entropy * entropy_coefficent).backward()\n",
    "                nn.utils.clip_grad_norm_(self.actor_critic.parameters(),\n",
    "                                         self.max_grad_norm)\n",
    "                self.optimizer.step()\n",
    "\n",
    "                value_loss_epoch += value_loss.item()\n",
    "                action_loss_epoch += action_loss.item()\n",
    "                dist_entropy_epoch += dist_entropy.item()\n",
    "\n",
    "        num_updates = self.ppo_epoch * self.num_mini_batch\n",
    "\n",
    "        value_loss_epoch /= num_updates\n",
    "        action_loss_epoch /= num_updates\n",
    "        dist_entropy_epoch /= num_updates\n",
    "\n",
    "        \"\"\"self.writer.add_scalar('value_loss', value_loss_epoch, j)\n",
    "        self.writer.add_scalar('policy_loss', action_loss_epoch, j)\n",
    "        self.writer.add_scalar('dist_entropy', dist_entropy_epoch, j)\"\"\"\n",
    "        \n",
    "        return value_loss_epoch, action_loss_epoch, dist_entropy_epoch  \n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, obs_shape, action_space, base, base_kwargs=None):\n",
    "        super(Policy, self).__init__()\n",
    "        if base_kwargs is None:\n",
    "            base_kwargs = {}\n",
    "\n",
    "        self.base = base(obs_shape[-1], **base_kwargs)\n",
    "        #print(\"action_space\", action_space.__class__.__name__)\n",
    "        #if action_space.__class__.__name__ == \"Discrete\":\n",
    "        num_outputs = NUM_ACTION\n",
    "        self.dist = Categorical(self.base.output_size, num_outputs)\n",
    "        #else:\n",
    "           # raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def is_recurrent(self):\n",
    "        return self.base.is_recurrent\n",
    "\n",
    "    @property\n",
    "    def recurrent_hidden_state_size(self):\n",
    "        \"\"\"Size of rnn_hx.\"\"\"\n",
    "        return self.base.recurrent_hidden_state_size\n",
    "\n",
    "    def forward(self, inputs, rnn_hxs, masks):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def act(self, inputs, rnn_hxs, masks, deterministic=False):\n",
    "        value, actor_features, rnn_hxs = self.base(inputs, rnn_hxs, masks)\n",
    "        dist = self.dist(actor_features)\n",
    "\n",
    "        if deterministic:\n",
    "            action = dist.mode()\n",
    "        else:\n",
    "            action = dist.sample()\n",
    "\n",
    "        action_log_probs = dist.log_probs(action)\n",
    "        #dist_entropy = dist.entropy().mean()\n",
    "\n",
    "        return value, action, action_log_probs, rnn_hxs\n",
    "\n",
    "    def get_value(self, inputs, rnn_hxs, masks):\n",
    "        value, _, _ = self.base(inputs, rnn_hxs, masks)\n",
    "        return value\n",
    "\n",
    "    def evaluate_actions(self, inputs, rnn_hxs, masks, action):\n",
    "        value, actor_features, rnn_hxs = self.base(inputs, rnn_hxs, masks)\n",
    "        dist = self.dist(actor_features)\n",
    "\n",
    "        action_log_probs = dist.log_probs(action)\n",
    "        dist_entropy = dist.entropy().mean()\n",
    "\n",
    "        return value, action_log_probs, dist_entropy, rnn_hxs   \n",
    "    \n",
    "\n",
    "\n",
    "def _flatten_helper(T, N, _tensor):\n",
    "    return _tensor.view(T * N, *_tensor.size()[2:]) \n",
    "class RolloutStorage(object):\n",
    "    def __init__(self, num_steps, num_processes, obs_shape, action_space,\n",
    "                 recurrent_hidden_state_size):\n",
    "        self.obs = torch.zeros(num_steps + 1, num_processes, *obs_shape)\n",
    "        self.recurrent_hidden_states = torch.zeros(\n",
    "            num_steps + 1, num_processes, recurrent_hidden_state_size)\n",
    "        self.rewards = torch.zeros(num_steps, num_processes, 1)\n",
    "        self.value_preds = torch.zeros(num_steps + 1, num_processes, 1)\n",
    "        self.returns = torch.zeros(num_steps + 1, num_processes, 1)\n",
    "        self.action_log_probs = torch.zeros(num_steps, num_processes, 1)\n",
    "        #if action_space.__class__.__name__ == 'Discrete':\n",
    "        action_shape = 1\n",
    "        #else:\n",
    "        #    action_shape = action_space.shape[0]\n",
    "        self.actions = torch.zeros(num_steps, num_processes, action_shape)\n",
    "        if action_space.__class__.__name__ == 'Discrete':\n",
    "            self.actions = self.actions.long()\n",
    "        self.masks = torch.ones(num_steps + 1, num_processes, 1)\n",
    "\n",
    "        # Masks that indicate whether it's a true terminal state\n",
    "        # or time limit end state\n",
    "        self.bad_masks = torch.ones(num_steps + 1, num_processes, 1)\n",
    "\n",
    "        self.num_steps = num_steps\n",
    "        self.step = 0\n",
    "\n",
    "    def to(self, device):\n",
    "        self.obs = self.obs.to(device)\n",
    "        self.recurrent_hidden_states = self.recurrent_hidden_states.to(device)\n",
    "        self.rewards = self.rewards.to(device)\n",
    "        self.value_preds = self.value_preds.to(device)\n",
    "        self.returns = self.returns.to(device)\n",
    "        self.action_log_probs = self.action_log_probs.to(device)\n",
    "        self.actions = self.actions.to(device)\n",
    "        self.masks = self.masks.to(device)\n",
    "        self.bad_masks = self.bad_masks.to(device)\n",
    "\n",
    "    def insert(self, obs, recurrent_hidden_states, actions, action_log_probs,\n",
    "               value_preds, rewards, masks, bad_masks):\n",
    "        self.obs[self.step + 1].copy_(obs)\n",
    "        self.recurrent_hidden_states[self.step +\n",
    "                                     1].copy_(recurrent_hidden_states)\n",
    "        self.actions[self.step].copy_(actions)\n",
    "        self.action_log_probs[self.step].copy_(action_log_probs)\n",
    "        self.value_preds[self.step].copy_(value_preds)\n",
    "        self.rewards[self.step].copy_(rewards)\n",
    "        self.masks[self.step + 1].copy_(masks)\n",
    "        self.bad_masks[self.step + 1].copy_(bad_masks)\n",
    "\n",
    "        self.step = (self.step + 1) % self.num_steps\n",
    "\n",
    "    def after_update(self):\n",
    "        self.obs[0].copy_(self.obs[-1])\n",
    "        self.recurrent_hidden_states[0].copy_(self.recurrent_hidden_states[-1])\n",
    "        self.masks[0].copy_(self.masks[-1])\n",
    "        self.bad_masks[0].copy_(self.bad_masks[-1])\n",
    "\n",
    "    def compute_returns(self,\n",
    "                        next_value,\n",
    "                        use_gae,\n",
    "                        gamma,\n",
    "                        gae_lambda,\n",
    "                        use_proper_time_limits=True):\n",
    "        if use_proper_time_limits:\n",
    "            if use_gae:\n",
    "                self.value_preds[-1] = next_value\n",
    "                gae = 0\n",
    "                for step in reversed(range(self.rewards.size(0))):\n",
    "                    delta = self.rewards[step] + gamma * self.value_preds[\n",
    "                        step + 1] * self.masks[step +\n",
    "                                               1] - self.value_preds[step]\n",
    "                    gae = delta + gamma * gae_lambda * self.masks[step +\n",
    "                                                                  1] * gae\n",
    "                    gae = gae * self.bad_masks[step + 1]\n",
    "                    self.returns[step] = gae + self.value_preds[step]\n",
    "            else:\n",
    "                self.returns[-1] = next_value\n",
    "                for step in reversed(range(self.rewards.size(0))):\n",
    "                    self.returns[step] = (self.returns[step + 1] * \\\n",
    "                        gamma * self.masks[step + 1] + self.rewards[step]) * self.bad_masks[step + 1] \\\n",
    "                        + (1 - self.bad_masks[step + 1]) * self.value_preds[step]\n",
    "        else:\n",
    "            if use_gae:\n",
    "                self.value_preds[-1] = next_value\n",
    "                gae = 0\n",
    "                for step in reversed(range(self.rewards.size(0))):\n",
    "                    delta = self.rewards[step] + gamma * self.value_preds[\n",
    "                        step + 1] * self.masks[step +\n",
    "                                               1] - self.value_preds[step]\n",
    "                    gae = delta + gamma * gae_lambda * self.masks[step +\n",
    "                                                                  1] * gae\n",
    "                    self.returns[step] = gae + self.value_preds[step]\n",
    "            else:\n",
    "                self.returns[-1] = next_value\n",
    "                for step in reversed(range(self.rewards.size(0))):\n",
    "                    self.returns[step] = self.returns[step + 1] * \\\n",
    "                        gamma * self.masks[step + 1] + self.rewards[step]\n",
    "\n",
    "\n",
    "    def feed_forward_generator(self,\n",
    "                               advantages,\n",
    "                               num_mini_batch=None,\n",
    "                               mini_batch_size=None):\n",
    "        num_steps, num_processes = self.rewards.size()[0:2]\n",
    "        batch_size = num_processes * num_steps\n",
    "\n",
    "        if mini_batch_size is None:\n",
    "            assert batch_size >= num_mini_batch, (\n",
    "                \"PPO requires the number of processes ({}) \"\n",
    "                \"* number of steps ({}) = {} \"\n",
    "                \"to be greater than or equal to the number of PPO mini batches ({}).\"\n",
    "                \"\".format(num_processes, num_steps, num_processes * num_steps,\n",
    "                          num_mini_batch))\n",
    "            mini_batch_size = batch_size // num_mini_batch\n",
    "        sampler = BatchSampler(\n",
    "            SubsetRandomSampler(range(batch_size)),\n",
    "            mini_batch_size,\n",
    "            drop_last=True)\n",
    "        for indices in sampler:\n",
    "            obs_batch = self.obs[:-1].view(-1, *self.obs.size()[2:])[indices]\n",
    "            recurrent_hidden_states_batch = self.recurrent_hidden_states[:-1].view(\n",
    "                -1, self.recurrent_hidden_states.size(-1))[indices]\n",
    "            actions_batch = self.actions.view(-1,\n",
    "                                              self.actions.size(-1))[indices]\n",
    "            value_preds_batch = self.value_preds[:-1].view(-1, 1)[indices]\n",
    "            return_batch = self.returns[:-1].view(-1, 1)[indices]\n",
    "            masks_batch = self.masks[:-1].view(-1, 1)[indices]\n",
    "            old_action_log_probs_batch = self.action_log_probs.view(-1,\n",
    "                                                                    1)[indices]\n",
    "            if advantages is None:\n",
    "                adv_targ = None\n",
    "            else:\n",
    "                adv_targ = advantages.view(-1, 1)[indices]\n",
    "\n",
    "            yield obs_batch, recurrent_hidden_states_batch, actions_batch, \\\n",
    "                value_preds_batch, return_batch, masks_batch, old_action_log_probs_batch, adv_targ\n",
    "\n",
    "    def recurrent_generator(self, advantages, num_mini_batch):\n",
    "        num_processes = self.rewards.size(1)\n",
    "        assert num_processes >= num_mini_batch, (\n",
    "            \"PPO requires the number of processes ({}) \"\n",
    "            \"to be greater than or equal to the number of \"\n",
    "            \"PPO mini batches ({}).\".format(num_processes, num_mini_batch))\n",
    "        num_envs_per_batch = num_processes // num_mini_batch\n",
    "        perm = torch.randperm(num_processes)\n",
    "        for start_ind in range(0, num_processes, num_envs_per_batch):\n",
    "            obs_batch = []\n",
    "            recurrent_hidden_states_batch = []\n",
    "            actions_batch = []\n",
    "            value_preds_batch = []\n",
    "            return_batch = []\n",
    "            masks_batch = []\n",
    "            old_action_log_probs_batch = []\n",
    "            adv_targ = []\n",
    "\n",
    "            for offset in range(num_envs_per_batch):\n",
    "                ind = perm[start_ind + offset]\n",
    "                obs_batch.append(self.obs[:-1, ind])\n",
    "                recurrent_hidden_states_batch.append(\n",
    "                    self.recurrent_hidden_states[0:1, ind])\n",
    "                actions_batch.append(self.actions[:, ind])\n",
    "                value_preds_batch.append(self.value_preds[:-1, ind])\n",
    "                return_batch.append(self.returns[:-1, ind])\n",
    "                masks_batch.append(self.masks[:-1, ind])\n",
    "                old_action_log_probs_batch.append(\n",
    "                    self.action_log_probs[:, ind])\n",
    "                adv_targ.append(advantages[:, ind])\n",
    "\n",
    "            T, N = self.num_steps, num_envs_per_batch\n",
    "            # These are all tensors of size (T, N, -1)\n",
    "            obs_batch = torch.stack(obs_batch, 1)\n",
    "            actions_batch = torch.stack(actions_batch, 1)\n",
    "            value_preds_batch = torch.stack(value_preds_batch, 1)\n",
    "            return_batch = torch.stack(return_batch, 1)\n",
    "            masks_batch = torch.stack(masks_batch, 1)\n",
    "            old_action_log_probs_batch = torch.stack(\n",
    "                old_action_log_probs_batch, 1)\n",
    "            adv_targ = torch.stack(adv_targ, 1)\n",
    "\n",
    "            # States is just a (N, -1) tensor\n",
    "            recurrent_hidden_states_batch = torch.stack(\n",
    "                recurrent_hidden_states_batch, 1).view(N, -1)\n",
    "\n",
    "            # Flatten the (T, N, ...) tensors to (T * N, ...)\n",
    "            obs_batch = _flatten_helper(T, N, obs_batch)\n",
    "            actions_batch = _flatten_helper(T, N, actions_batch)\n",
    "            value_preds_batch = _flatten_helper(T, N, value_preds_batch)\n",
    "            return_batch = _flatten_helper(T, N, return_batch)\n",
    "            masks_batch = _flatten_helper(T, N, masks_batch)\n",
    "            old_action_log_probs_batch = _flatten_helper(T, N, \\\n",
    "                    old_action_log_probs_batch)\n",
    "            adv_targ = _flatten_helper(T, N, adv_targ)\n",
    "\n",
    "            yield obs_batch, recurrent_hidden_states_batch, actions_batch, \\\n",
    "                value_preds_batch, return_batch, masks_batch, old_action_log_probs_batch, adv_targ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "actor_critic = Policy(\n",
    "        obs_shape=(3, ENV_DIM, ENV_DIM, ENV_DIM),\n",
    "        action_space=NUM_ACTION,\n",
    "        base=CNNBase,\n",
    "        )\n",
    "\n",
    "actor_critic = torch.load(\"A2c_checkpoint_600.pt\")\n",
    "actor_critic.to(device)\n",
    "    \n",
    "    \n",
    "agent = PPO(\n",
    "            actor_critic,\n",
    "            clip_param=0.2,\n",
    "            ppo_epoch=10,\n",
    "            num_mini_batch=NUM_PROCESSES,\n",
    "            value_loss_coef=VALUE_COEFF,\n",
    "            entropy_coef=ENTROPY_COEFF,\n",
    "            lr=LR,\n",
    "            eps=1e-8,\n",
    "            max_grad_norm=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False]\n",
      "tensor([[1.4987e-01, 3.0802e-01, 1.3494e-01, 2.1783e-01, 9.6138e-02, 9.3170e-02,\n",
      "         3.0387e-05, 6.4340e-06],\n",
      "        [1.6048e-01, 1.2149e-01, 1.6287e-01, 3.5801e-01, 1.1755e-01, 7.9595e-02,\n",
      "         1.8626e-06, 3.4016e-07]], device='cuda:0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 52\u001b[0m\n\u001b[0;32m     45\u001b[0m bad_masks \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(\n\u001b[0;32m     46\u001b[0m     [[\u001b[38;5;241m0.0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m done_ \u001b[38;5;28;01melse\u001b[39;00m [\u001b[38;5;241m1.0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m done_ \u001b[38;5;129;01min\u001b[39;00m done])\n\u001b[0;32m     49\u001b[0m rollouts\u001b[38;5;241m.\u001b[39minsert(obs, recurrent_hidden_states, action,\n\u001b[0;32m     50\u001b[0m                 action_log_prob, value, reward, masks, bad_masks)\n\u001b[1;32m---> 52\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m display\u001b[38;5;241m.\u001b[39mclear_output(wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(done)\n",
      "File \u001b[1;32mc:\\Users\\tutha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:70\u001b[0m, in \u001b[0;36mOrderEnforcing.render\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_render_order_enforcing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\n\u001b[0;32m     67\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call `env.render()` before calling `env.reset()`, if this is a intended action, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     69\u001b[0m     )\n\u001b[1;32m---> 70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tutha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:67\u001b[0m, in \u001b[0;36mPassiveEnvChecker.render\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_render_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\tutha\\desktop\\cmput469\\a2c\\deep-reinforcement-learning-agent-for-construction-project-execution\\gridworld-env\\GridWorld_env\\envs\\grid_world.py:255\u001b[0m, in \u001b[0;36mGridWorldEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    243\u001b[0m     ax\u001b[38;5;241m.\u001b[39mvoxels(building_zone_render, facecolors\u001b[38;5;241m=\u001b[39mcolors, edgecolor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    245\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"col_cube = self.target == GridWorldEnv.COL_BLOCK\u001b[39;00m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;124;03mbeam_cube = self.target == GridWorldEnv.BEAM_BLOCK\u001b[39;00m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;124;03mtarget_render = col_cube | beam_cube\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;124;03max = fig.add_subplot(1, 2, 2, projection='3d')\u001b[39;00m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;124;03max.voxels(target_render, facecolors=colors, edgecolor='k')\"\"\"\u001b[39;00m\n\u001b[1;32m--> 255\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tutha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\pyplot.py:446\u001b[0m, in \u001b[0;36mshow\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;124;03mDisplay all open figures.\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;124;03mexplicitly there.\u001b[39;00m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    445\u001b[0m _warn_if_gui_out_of_main_thread()\n\u001b[1;32m--> 446\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_backend_mod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\matplotlib_inline\\backend_inline.py:90\u001b[0m, in \u001b[0;36mshow\u001b[1;34m(close, block)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m figure_manager \u001b[38;5;129;01min\u001b[39;00m Gcf\u001b[38;5;241m.\u001b[39mget_all_fig_managers():\n\u001b[1;32m---> 90\u001b[0m         \u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfigure_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fetch_figure_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfigure_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     show\u001b[38;5;241m.\u001b[39m_to_draw \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\display_functions.py:298\u001b[0m, in \u001b[0;36mdisplay\u001b[1;34m(include, exclude, metadata, transient, display_id, raw, clear, *objs, **kwargs)\u001b[0m\n\u001b[0;32m    296\u001b[0m     publish_display_data(data\u001b[38;5;241m=\u001b[39mobj, metadata\u001b[38;5;241m=\u001b[39mmetadata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 298\u001b[0m     format_dict, md_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m format_dict:\n\u001b[0;32m    300\u001b[0m         \u001b[38;5;66;03m# nothing to display (e.g. _ipython_display_ took over)\u001b[39;00m\n\u001b[0;32m    301\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\formatters.py:177\u001b[0m, in \u001b[0;36mDisplayFormatter.format\u001b[1;34m(self, obj, include, exclude)\u001b[0m\n\u001b[0;32m    175\u001b[0m md \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 177\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;66;03m# FIXME: log the exception\u001b[39;00m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[0;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcaller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mextras\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\formatters.py:221\u001b[0m, in \u001b[0;36mcatch_format_error\u001b[1;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"show traceback on failed format call\"\"\"\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 221\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;66;03m# don't warn on NotImplementedErrors\u001b[39;00m\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_return(\u001b[38;5;28;01mNone\u001b[39;00m, args[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\formatters.py:338\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 338\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprinter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;66;03m# Finally look for special method names\u001b[39;00m\n\u001b[0;32m    340\u001b[0m method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\pylabtools.py:152\u001b[0m, in \u001b[0;36mprint_figure\u001b[1;34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_bases\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FigureCanvasBase\n\u001b[0;32m    150\u001b[0m     FigureCanvasBase(fig)\n\u001b[1;32m--> 152\u001b[0m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_figure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbytes_io\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    153\u001b[0m data \u001b[38;5;241m=\u001b[39m bytes_io\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fmt \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\tutha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\backend_bases.py:2346\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[1;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[0;32m   2344\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bbox_inches:\n\u001b[0;32m   2345\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bbox_inches \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtight\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 2346\u001b[0m         bbox_inches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_tightbbox\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2347\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox_extra_artists\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbbox_extra_artists\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2348\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m pad_inches \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2349\u001b[0m             pad_inches \u001b[38;5;241m=\u001b[39m rcParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msavefig.pad_inches\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\tutha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\figure.py:1744\u001b[0m, in \u001b[0;36mFigureBase.get_tightbbox\u001b[1;34m(self, renderer, bbox_extra_artists)\u001b[0m\n\u001b[0;32m   1741\u001b[0m     artists \u001b[38;5;241m=\u001b[39m bbox_extra_artists\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m artists:\n\u001b[1;32m-> 1744\u001b[0m     bbox \u001b[38;5;241m=\u001b[39m \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_tightbbox\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bbox \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1746\u001b[0m         bb\u001b[38;5;241m.\u001b[39mappend(bbox)\n",
      "File \u001b[1;32mc:\\Users\\tutha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mpl_toolkits\\mplot3d\\axes3d.py:3197\u001b[0m, in \u001b[0;36mAxes3D.get_tightbbox\u001b[1;34m(self, renderer, call_axes_locator, bbox_extra_artists, for_layout_only)\u001b[0m\n\u001b[0;32m   3195\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_axis_map\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m   3196\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m axis\u001b[38;5;241m.\u001b[39mget_visible():\n\u001b[1;32m-> 3197\u001b[0m         axis_bb \u001b[38;5;241m=\u001b[39m \u001b[43mmartist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_tightbbox_for_layout_only\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3198\u001b[0m \u001b[43m            \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3199\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m axis_bb:\n\u001b[0;32m   3200\u001b[0m             batch\u001b[38;5;241m.\u001b[39mappend(axis_bb)\n",
      "File \u001b[1;32mc:\\Users\\tutha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\artist.py:1415\u001b[0m, in \u001b[0;36m_get_tightbbox_for_layout_only\u001b[1;34m(obj, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1409\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1410\u001b[0m \u001b[38;5;124;03mMatplotlib's `.Axes.get_tightbbox` and `.Axis.get_tightbbox` support a\u001b[39;00m\n\u001b[0;32m   1411\u001b[0m \u001b[38;5;124;03m*for_layout_only* kwarg; this helper tries to use the kwarg but skips it\u001b[39;00m\n\u001b[0;32m   1412\u001b[0m \u001b[38;5;124;03mwhen encountering third-party subclasses that do not support it.\u001b[39;00m\n\u001b[0;32m   1413\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1415\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_tightbbox\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfor_layout_only\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   1417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mget_tightbbox(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tutha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\mpl_toolkits\\mplot3d\\axis3d.py:567\u001b[0m, in \u001b[0;36mAxis.get_tightbbox\u001b[1;34m(self, renderer, for_layout_only)\u001b[0m\n\u001b[0;32m    563\u001b[0m             ticks_to_draw\u001b[38;5;241m.\u001b[39mappend(tick)\n\u001b[0;32m    565\u001b[0m ticks \u001b[38;5;241m=\u001b[39m ticks_to_draw\n\u001b[1;32m--> 567\u001b[0m bb_1, bb_2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_ticklabel_bboxes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mticks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    568\u001b[0m other \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    570\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mline\u001b[38;5;241m.\u001b[39mget_visible():\n",
      "File \u001b[1;32mc:\\Users\\tutha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\axis.py:1304\u001b[0m, in \u001b[0;36mAxis._get_ticklabel_bboxes\u001b[1;34m(self, ticks, renderer)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m renderer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1303\u001b[0m     renderer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure\u001b[38;5;241m.\u001b[39m_get_renderer()\n\u001b[1;32m-> 1304\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[43m[\u001b[49m\u001b[43mtick\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_window_extent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1305\u001b[0m \u001b[43m         \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtick\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mticks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtick\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_visible\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m,\n\u001b[0;32m   1306\u001b[0m         [tick\u001b[38;5;241m.\u001b[39mlabel2\u001b[38;5;241m.\u001b[39mget_window_extent(renderer)\n\u001b[0;32m   1307\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m tick \u001b[38;5;129;01min\u001b[39;00m ticks \u001b[38;5;28;01mif\u001b[39;00m tick\u001b[38;5;241m.\u001b[39mlabel2\u001b[38;5;241m.\u001b[39mget_visible()])\n",
      "File \u001b[1;32mc:\\Users\\tutha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\axis.py:1304\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m renderer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1303\u001b[0m     renderer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure\u001b[38;5;241m.\u001b[39m_get_renderer()\n\u001b[1;32m-> 1304\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ([\u001b[43mtick\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_window_extent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1305\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m tick \u001b[38;5;129;01min\u001b[39;00m ticks \u001b[38;5;28;01mif\u001b[39;00m tick\u001b[38;5;241m.\u001b[39mlabel1\u001b[38;5;241m.\u001b[39mget_visible()],\n\u001b[0;32m   1306\u001b[0m         [tick\u001b[38;5;241m.\u001b[39mlabel2\u001b[38;5;241m.\u001b[39mget_window_extent(renderer)\n\u001b[0;32m   1307\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m tick \u001b[38;5;129;01min\u001b[39;00m ticks \u001b[38;5;28;01mif\u001b[39;00m tick\u001b[38;5;241m.\u001b[39mlabel2\u001b[38;5;241m.\u001b[39mget_visible()])\n",
      "File \u001b[1;32mc:\\Users\\tutha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\text.py:961\u001b[0m, in \u001b[0;36mText.get_window_extent\u001b[1;34m(self, renderer, dpi)\u001b[0m\n\u001b[0;32m    959\u001b[0m bbox, info, descent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_layout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_renderer)\n\u001b[0;32m    960\u001b[0m x, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_unitless_position()\n\u001b[1;32m--> 961\u001b[0m x, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    962\u001b[0m bbox \u001b[38;5;241m=\u001b[39m bbox\u001b[38;5;241m.\u001b[39mtranslated(x, y)\n\u001b[0;32m    963\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m bbox\n",
      "File \u001b[1;32mc:\\Users\\tutha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\transforms.py:1490\u001b[0m, in \u001b[0;36mTransform.transform\u001b[1;34m(self, values)\u001b[0m\n\u001b[0;32m   1487\u001b[0m values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_dims))\n\u001b[0;32m   1489\u001b[0m \u001b[38;5;66;03m# Transform the values\u001b[39;00m\n\u001b[1;32m-> 1490\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform_affine\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform_non_affine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1492\u001b[0m \u001b[38;5;66;03m# Convert the result back to the shape of the input values.\u001b[39;00m\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\tutha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\transforms.py:2415\u001b[0m, in \u001b[0;36mCompositeGenericTransform.transform_affine\u001b[1;34m(self, points)\u001b[0m\n\u001b[0;32m   2413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform_affine\u001b[39m(\u001b[38;5;28mself\u001b[39m, points):\n\u001b[0;32m   2414\u001b[0m     \u001b[38;5;66;03m# docstring inherited\u001b[39;00m\n\u001b[1;32m-> 2415\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_affine\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(points)\n",
      "File \u001b[1;32mc:\\Users\\tutha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\transforms.py:2441\u001b[0m, in \u001b[0;36mCompositeGenericTransform.get_affine\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_b\u001b[38;5;241m.\u001b[39mget_affine()\n\u001b[0;32m   2440\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2441\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Affine2D(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_b\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_affine\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2442\u001b[0m \u001b[43m                           \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_a\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_affine\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rollouts = RolloutStorage(num_steps=NUM_STEP, num_processes=NUM_PROCESSES,\n",
    "                              obs_shape=(3, ENV_DIM, ENV_DIM, ENV_DIM), action_space=NUM_ACTION,\n",
    "                              recurrent_hidden_state_size=RECURRENT_HIDDEN_SIZE)\n",
    "\n",
    "from IPython import display\n",
    "import GridWorld_env\n",
    "import gymnasium as gym\n",
    "\n",
    "n_actions = 8\n",
    "env = gym.make(\"GridWorld_env/GridWorld\", dimension_size=4, path=\"targets\", batch_size=NUM_PROCESSES)\n",
    "env.reset()\n",
    "\n",
    "obs = env.get_obs()\n",
    "obs = torch.from_numpy(obs).float()\n",
    "rollouts.obs[0].copy_(obs)\n",
    "rollouts.to(device)\n",
    "\n",
    "\"\"\"if USE_LR_DECAY:\n",
    "    # decrease learning rate linearly\n",
    "    utils.update_linear_schedule(\n",
    "        agent.optimizer, j, num_updates,\n",
    "        agent.optimizer.lr if args.algo == \"acktr\" else args.lr)\"\"\"\n",
    "\n",
    "for step in range(NUM_STEP):\n",
    "    # Sample actions\n",
    "    with torch.no_grad():\n",
    "        value, action, action_log_prob, recurrent_hidden_states = actor_critic.act(\n",
    "            rollouts.obs[step], rollouts.recurrent_hidden_states[step],\n",
    "            rollouts.masks[step])\n",
    "    #print(rollouts.obs[step].shape, rollouts.recurrent_hidden_states[step].shape, rollouts.masks[step].shape)\n",
    "    # Obser reward and next obs\n",
    "    obs, reward, done, truncated, infos = env.step(action)\n",
    "    obs = torch.from_numpy(obs).float()\n",
    "    reward = torch.from_numpy(reward).unsqueeze(-1).float()\n",
    "    #done = [done]\n",
    "    #episode_rewards.append(reward)\n",
    "\n",
    "    # If done then clean the history of observations.\n",
    "    masks = torch.FloatTensor(\n",
    "        [[0.0] if done_ else [1.0] for done_ in done])\n",
    "    \"\"\"bad_masks = torch.FloatTensor(\n",
    "        [[0.0] if 'bad_transition' in info.keys() else [1.0]\n",
    "            for info in infos])\"\"\"\n",
    "            \n",
    "    bad_masks = torch.FloatTensor(\n",
    "        [[0.0] if done_ else [1.0] for done_ in done])\n",
    "    \n",
    "    \n",
    "    rollouts.insert(obs, recurrent_hidden_states, action,\n",
    "                    action_log_prob, value, reward, masks, bad_masks)\n",
    "    \n",
    "    env.render()\n",
    "    display.clear_output(wait=True)\n",
    "    print(done)\n",
    "\n",
    "with torch.no_grad():\n",
    "    next_value = actor_critic.get_value(\n",
    "        rollouts.obs[-1], rollouts.recurrent_hidden_states[-1],\n",
    "        rollouts.masks[-1]).detach()\n",
    "#plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
