{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import GridWorld_env\n",
    "from replay_buffer import ReplayBuffer, PrioritizedReplayBuffer\n",
    "import gymnasium as gym\n",
    "import random\n",
    "import math\n",
    "from itertools import count\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "is_ipython = \"inline\" in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "    \n",
    "from collections import deque, namedtuple\n",
    "\n",
    "Transition = namedtuple(\"Transition\", (\"state\", \"action\", \"next_state\", \"reward\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vanilla_DQN(nn.Module):\n",
    "    def __init__(self, input_dim, action_dim):\n",
    "        super(Vanilla_DQN, self).__init__()\n",
    "               \n",
    "        self.conv1 = nn.Conv3d(3, 29, 3, 1, 1)\n",
    "        \n",
    "        self.conv2 = nn.Conv3d(32, 67, 3, 1, 1)\n",
    "        \n",
    "        self.fc1 = nn.Linear((64 + 3*2)*input_dim * input_dim * input_dim, 1024)\n",
    "        \n",
    "        self.actions = nn.Linear(1024, action_dim)\n",
    "        self.advantage = nn.Linear(1024, 1)\n",
    "    def forward(self, x):\n",
    "        original_state = x\n",
    "               \n",
    "        x = self.conv1(x)\n",
    "        \n",
    "        x = torch.cat([x, original_state], dim=1)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = torch.cat([x, original_state], dim=1)\n",
    "        \n",
    "        x = F.relu(x)\n",
    "        x = nn.Flatten()(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        actions = self.actions(x)\n",
    "        \n",
    "        x = self.advantage(x) + (actions - actions.mean(dim=1, keepdim=True))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tutha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.9\n",
    "EPS_START = 0.99\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = 150000\n",
    "TAU = 0.0003\n",
    "STEPSIZE = 0.0000625\n",
    "BETA_START = 0.4\n",
    "BETA_END = 1\n",
    "BETA_LINEAR_CAP = 850 * 850\n",
    "N_STEP = 1\n",
    "\n",
    "n_actions = 8\n",
    "env = gym.make(\"GridWorld_env/GridWorld\", dimension_size=4, path=\"targets\")\n",
    "env.reset()\n",
    "\n",
    "policy_net = Vanilla_DQN(4, 8)\n",
    "target_net = Vanilla_DQN(4, 8)\n",
    "\n",
    "\n",
    "policy_net.cuda()\n",
    "target_net.cuda()\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimiser = optim.Adam(policy_net.parameters(), lr=STEPSIZE, eps=1.5e-4)\n",
    "memory = PrioritizedReplayBuffer(obs_dim=(3,4,4,4), size=8192, n_step=N_STEP, gamma = GAMMA)\n",
    "\n",
    "steps_done = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, greedy = False):\n",
    "    global steps_done\n",
    "    \n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    \n",
    "    if greedy:\n",
    "        return policy_net(state).max(1).indices.view(1,1)\n",
    "        \n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1).indices.view(1,1)\n",
    "    else:\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_durations = []\n",
    "\n",
    "def plot_durations(show_result = False):\n",
    "    plt.figure(1)\n",
    "    \n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title(\"Result\")\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title(\"Training\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Duration\")\n",
    "    \n",
    "    plt.plot(durations_t.numpy())\n",
    "    \n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "        \n",
    "        plt.pause(0.001)\n",
    "        if is_ipython:\n",
    "            if not show_result:\n",
    "                display.display(plt.gcf())\n",
    "                display.clear_output(wait=True)\n",
    "            else:\n",
    "                display.display(plt.gcf())\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimise_model(beta):\n",
    "    if len(memory) < BATCH_SIZE * 64:\n",
    "        return 0, 0\n",
    "    # transitions = memory.sample(BATCH_SIZE)\n",
    "    transitions = memory.sample_batch(beta)\n",
    "    # batch = Transition(*zip(*transitions))\n",
    "    # batch = []\n",
    "    batch = Transition(\n",
    "        torch.tensor(transitions[\"obs\"], device=device),\n",
    "        torch.tensor(transitions[\"acts\"], device=device, dtype=torch.int64),\n",
    "        torch.tensor(transitions[\"next_obs\"], device=device),\n",
    "        torch.tensor(transitions[\"rews\"], device=device)\n",
    "    )\n",
    "    \n",
    "    indices = transitions[\"indices\"]\n",
    "    \n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s.unsqueeze(0) for s in batch.next_state if s is not None])\n",
    "    \n",
    "    state_batch = batch.state\n",
    "    action_batch = batch.action\n",
    "    reward_batch = batch.reward\n",
    "    \n",
    "    tmp = policy_net(state_batch)\n",
    "    state_action_values = tmp.gather(1, action_batch.unsqueeze(1))\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device = device)\n",
    "    \n",
    "    #double dqn\n",
    "    max_a = policy_net(non_final_next_states).detach().max(1).indices\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).detach().gather(1, max_a.unsqueeze(1)).squeeze(1)\n",
    "    #next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
    "    with torch.no_grad():\n",
    "        expected_state_action_values = (GAMMA ** N_STEP) * next_state_values + reward_batch\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    weights = torch.tensor(transitions[\"weights\"], device=device)\n",
    "    weights = weights ** 0.5\n",
    "    \n",
    "    loss = criterion(weights * state_action_values , weights * expected_state_action_values.unsqueeze(1))\n",
    "    \n",
    "    optimiser.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimiser.step()\n",
    "    \n",
    "    new_priorities = torch.abs(state_action_values - expected_state_action_values.unsqueeze(1)).detach().cpu().numpy() + 1e-7 #loss.detach().cpu().numpy() + 1e-6\n",
    "    #print(len(indices), new_priorities.shape, new_priorities)\n",
    "    memory.update_priorities(indices, new_priorities)\n",
    "    return loss.item(), reward_batch.float()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_plot = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tutha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m     30\u001b[0m beta \u001b[38;5;241m=\u001b[39m BETA_START \u001b[38;5;241m+\u001b[39m (BETA_END \u001b[38;5;241m-\u001b[39m BETA_START) \u001b[38;5;241m*\u001b[39m (steps_done) \u001b[38;5;241m/\u001b[39m BETA_LINEAR_CAP \u001b[38;5;28;01mif\u001b[39;00m steps_done \u001b[38;5;241m<\u001b[39m BETA_LINEAR_CAP \u001b[38;5;28;01melse\u001b[39;00m BETA_END\n\u001b[1;32m---> 31\u001b[0m l, r \u001b[38;5;241m=\u001b[39m \u001b[43moptimise_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Soft update of the target network's weights\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# θ′ ← τ θ + (1 −τ )θ′\u001b[39;00m\n\u001b[0;32m     35\u001b[0m target_net_state_dict \u001b[38;5;241m=\u001b[39m target_net\u001b[38;5;241m.\u001b[39mstate_dict()\n",
      "Cell \u001b[1;32mIn[6], line 27\u001b[0m, in \u001b[0;36moptimise_model\u001b[1;34m(beta)\u001b[0m\n\u001b[0;32m     23\u001b[0m next_state_values \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(BATCH_SIZE, device \u001b[38;5;241m=\u001b[39m device)\n\u001b[0;32m     26\u001b[0m max_a \u001b[38;5;241m=\u001b[39m policy_net(non_final_next_states)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mindices\n\u001b[1;32m---> 27\u001b[0m next_state_values[non_final_mask] \u001b[38;5;241m=\u001b[39m \u001b[43mtarget_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnon_final_next_states\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m1\u001b[39m, max_a\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     29\u001b[0m     expected_state_action_values \u001b[38;5;241m=\u001b[39m (GAMMA \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m N_STEP) \u001b[38;5;241m*\u001b[39m next_state_values \u001b[38;5;241m+\u001b[39m reward_batch\n",
      "File \u001b[1;32mc:\\Users\\tutha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[2], line 28\u001b[0m, in \u001b[0;36mVanilla_DQN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     26\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x)\n\u001b[0;32m     27\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[1;32m---> 28\u001b[0m actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvantage(x) \u001b[38;5;241m+\u001b[39m (actions \u001b[38;5;241m-\u001b[39m actions\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m actions\n",
      "File \u001b[1;32mc:\\Users\\tutha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\tutha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    num_episodes = 600\n",
    "else:\n",
    "    num_episodes = 1\n",
    "\n",
    "#num_actions = torch.zeros(n_actions, device=device)\n",
    "\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and get its state\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    cummulative_reward = 0\n",
    "    for t in count():\n",
    "        action = select_action(state)\n",
    "\n",
    "        #num_actions[action] +=1\n",
    "\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        \n",
    "        cummulative_reward += reward\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        \n",
    "        done = terminated or truncated\n",
    "        if terminated:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # Store the transition in memory\n",
    "        # memory.push(state, action, next_state, reward)\n",
    "        memory.store(state.cpu().numpy(), action, reward, next_state.cpu().numpy(), done)\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        beta = BETA_START + (BETA_END - BETA_START) * (steps_done) / BETA_LINEAR_CAP if steps_done < BETA_LINEAR_CAP else BETA_END\n",
    "        l, r = optimise_model(beta)\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "        \n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            break\n",
    "    #if i_episode % 25 == 0:\n",
    "        #target_net.load_state_dict(policy_net.state_dict())\n",
    "        \n",
    "    if i_episode % 1 == 0 and i_episode > 1:\n",
    "        print(\"Episode: {0} Loss {1} Mean Sample Reward {2}:\" .format(i_episode, l, r.mean().item()))\n",
    "        #print(env.unwrapped.get_seqsuence())\n",
    "        env.unwrapped.render()\n",
    "    \n",
    "    reward_plot.append(cummulative_reward)\n",
    "\n",
    "print('Complete')\n",
    "plot_durations(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, info = env.reset()\n",
    "state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "for t in range(50):\n",
    "    action = select_action(state, greedy = True)\n",
    "    observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "    reward = torch.tensor([reward], device=device)\n",
    "    done = terminated or truncated\n",
    "\n",
    "    if terminated:\n",
    "        next_state = None\n",
    "    else:\n",
    "        next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "    # Store the transition in memory\n",
    "    #memory.push(state, action, next_state, reward)\n",
    "\n",
    "    # Move to the next state\n",
    "    state = next_state\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "    env.unwrapped.render()\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy_net, \"vanilla_dqn_policy.pt\")\n",
    "torch.save(target_net, \"vanilla_dqn_target.pt\")\n",
    "\n",
    "np.save(\"duration_plot.npy\", np.array(episode_durations))\n",
    "np.save(\"reward_plot.npy\", np.array(reward_plot))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
